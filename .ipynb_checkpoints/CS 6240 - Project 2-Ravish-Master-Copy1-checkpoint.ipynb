{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>LSTM Language Detection</center></h1>\n",
    "<h3><center>CSE 6240 - Websearch and Text Mining</center></h3>\n",
    "\n",
    "<h7><center>Adam Lieberman, Garrett Mallory, Ravish Chawla</center></h7>\n",
    "<h7><center>April 25, 2017</center></h7>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) make 80/20 split on end and frn languages. (lower case all letters)\n",
    "2) create two lstm models (size 128)\n",
    "3) train 5 epochs, 0.2 validation split on the training samples \n",
    "4) \n",
    "4) train on 100 examples of 5 character strings from text with the right label.\n",
    "\n",
    "pass new word through lstm to get log probability. then do generative log ratio test. y = sign(pe/pj -1) to get the label. Keep scores for ROC.\n",
    "\n",
    "to get log probability, need p(t), p(r|t), p(u|tr) ... etc. Do this by passing in t, then tr then tru .... This gives you the most likely next character distribution so index into that to get the prob of \"U\" from \"tr\".\n",
    "\n",
    "pass both sets of 100 test (all 200) through both networks for the probability part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function;\n",
    "import re;\n",
    "import sys;\n",
    "import random;\n",
    "import unicodedata;\n",
    "from random import randint;\n",
    "\n",
    "import numpy as np;\n",
    "\n",
    "from keras.models import Sequential;\n",
    "from keras.layers import Dense, Activation;\n",
    "from keras.layers import LSTM;\n",
    "from keras.optimizers import RMSprop;\n",
    "from keras.utils.data_utils import get_file;\n",
    "from sklearn.cross_validation import train_test_split;\n",
    "from sklearn.metrics import *;\n",
    "from sklearn.externals import joblib;\n",
    "\n",
    "import matplotlib.pyplot as plt;\n",
    "from IPython.display import clear_output\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "\n",
    "import subprocess;\n",
    "import h5py;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def notify_slack(text):\n",
    "    text = 'WebSearch: ' + text;\n",
    "    subprocess.Popen('''curl -X POST --data-urlencode \"payload={'channel' : '#random', 'username': 'webhookbot', 'text':'''+ '\\'' + text + '\\'' + '''}\" https://hooks.slack.com/services/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''', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "universal declaratio\n",
      "declaration universe\n",
      "universele verklarin\n",
      "resolution 217 a (ii\n",
      "il 10 dicembre 1948,\n",
      "declarationem homini\n",
      "declaracao universal\n",
      "'sekai jinken sengen\n",
      "vseobsaja deklaracij\n",
      "declaracion universa\n"
     ]
    }
   ],
   "source": [
    "languages = ['data/eng.txt',\n",
    "             'data/frn.txt',\n",
    "             'languages/dut.txt',\n",
    "             'languages/ger.txt', \n",
    "             'languages/itn.txt', \n",
    "             'languages/ltn.txt', \n",
    "             'languages/por.txt', \n",
    "             'languages/romanized_jap.txt', \n",
    "             'languages/romanized_rus.txt', \n",
    "             'languages/spn.txt' ]\n",
    "\n",
    "# Hardest:\n",
    "# Ravish says Latin to Italian or Spanish - French\n",
    "# Garrett says German - English or Port - Spanish\n",
    "# Adam says Port - Latin or English - Italian\n",
    "\n",
    "# Easiest:\n",
    "# Japanese - Russian\n",
    "\n",
    "language_names = ['English',\n",
    "                  'French',\n",
    "                  'Dutch',\n",
    "                  'German',\n",
    "                  'Italian',\n",
    "                  'Latin',\n",
    "                  'Portugese',\n",
    "                  'Japanese',\n",
    "                  'Russian',\n",
    "                  'Spanish']\n",
    "\n",
    "all_text = []\n",
    "for file in languages:\n",
    "    text = open(file).read().lower()\n",
    "    all_text.append(strip_accents(text))\n",
    "\n",
    "for text in all_text:\n",
    "    print(text[0:20])\n",
    "\n",
    "    \n",
    "# Used this link to Romanize Russian: http://www.lexilogos.com/keyboard/russian_conversion.htm\n",
    "# Used this link to Ramanized Japanese: http://nihongo.j-talk.com/\n",
    "# additionally stripped out these non-translated characters:\n",
    "#not present: 及\n",
    "#not present: 受\n",
    "#not present: 胞\n",
    "#not present: 認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "\tCorpus length: 10746\n",
      "\tCharacter Count 43\n",
      "\n",
      "French\n",
      "\tCorpus length: 12009\n",
      "\tCharacter Count 41\n",
      "\n",
      "Dutch\n",
      "\tCorpus length: 12887\n",
      "\tCharacter Count 42\n",
      "\n",
      "German\n",
      "\tCorpus length: 12078\n",
      "\tCharacter Count 45\n",
      "\n",
      "Italian\n",
      "\tCorpus length: 12768\n",
      "\tCharacter Count 39\n",
      "\n",
      "Latin\n",
      "\tCorpus length: 10065\n",
      "\tCharacter Count 32\n",
      "\n",
      "Portugese\n",
      "\tCorpus length: 11459\n",
      "\tCharacter Count 42\n",
      "\n",
      "Japanese\n",
      "\tCorpus length: 11551\n",
      "\tCharacter Count 38\n",
      "\n",
      "Russian\n",
      "\tCorpus length: 12240\n",
      "\tCharacter Count 43\n",
      "\n",
      "Spanish\n",
      "\tCorpus length: 12078\n",
      "\tCharacter Count 42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_chars = []\n",
    "all_char_map = []\n",
    "all_char_map_inverse = []\n",
    "for idx, lang in enumerate(all_text):\n",
    "    all_chars.append(sorted(list(set(lang))))\n",
    "    all_char_map.append(dict((c, i) for i, c in enumerate(sorted(list(set(lang))))))\n",
    "    all_char_map_inverse.append(dict((i, c) for i, c in enumerate(sorted(list(set(lang))))))\n",
    "    \n",
    "    print (language_names[idx])\n",
    "    print (\"\\tCorpus length:\", len(lang))\n",
    "    print (\"\\tCharacter Count\", len(all_chars[idx]))\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "\tSequences: 3569\n",
      "\n",
      "French\n",
      "\tSequences: 3990\n",
      "\n",
      "Dutch\n",
      "\tSequences: 4283\n",
      "\n",
      "German\n",
      "\tSequences: 4013\n",
      "\n",
      "Italian\n",
      "\tSequences: 4243\n",
      "\n",
      "Latin\n",
      "\tSequences: 3342\n",
      "\n",
      "Portugese\n",
      "\tSequences: 3807\n",
      "\n",
      "Japanese\n",
      "\tSequences: 3837\n",
      "\n",
      "Russian\n",
      "\tSequences: 4067\n",
      "\n",
      "Spanish\n",
      "\tSequences: 4013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "\n",
    "all_sentences = []\n",
    "all_next_chars = []\n",
    "\n",
    "for idx, lang in enumerate(all_text):\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(lang) - maxlen, step):\n",
    "        sentences.append(lang[i: i + maxlen])\n",
    "        next_chars.append(lang[i + maxlen])\n",
    "    \n",
    "    all_sentences.append(sentences)\n",
    "    all_next_chars.append(next_chars)\n",
    "\n",
    "    print (language_names[idx])\n",
    "    print (\"\\tSequences:\", len(sentences))\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "\n",
    "char_len = max([len(x) for x in all_chars])\n",
    "\n",
    "all_x = []\n",
    "all_y = []\n",
    "\n",
    "for idx, lang in enumerate(all_text):\n",
    "    x = np.zeros((len(all_sentences[idx]), maxlen, char_len), dtype=np.bool)\n",
    "    y = np.zeros((len(all_sentences[idx]), char_len), dtype=np.bool)\n",
    "    \n",
    "    for i, sentence in enumerate(all_sentences[idx]):\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[i, t, all_char_map[idx][char]] = 1\n",
    "        y[i, all_char_map[idx][all_next_chars[idx][i]]] = 1\n",
    "    \n",
    "    all_x.append(x)\n",
    "    all_y.append(y)\n",
    "\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_train_x = []\n",
    "all_test_x = []\n",
    "all_train_y = []\n",
    "all_test_y =[]\n",
    "\n",
    "for idx, lang in enumerate(all_text):\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state=1024);\n",
    "    all_train_x.append(train_x)\n",
    "    all_test_x.append(test_x)\n",
    "    all_train_y.append(train_y)\n",
    "    all_test_y.append(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(Callback):\n",
    "\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.all = {};\n",
    "        self.all['acc'] = [];\n",
    "        self.all['val_acc'] = [];\n",
    "        self.all['loss'] = [];\n",
    "        self.all['val_loss'] = [];\n",
    "        pass;\n",
    "    \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.all['acc'].append(logs['acc'])\n",
    "        self.all['val_acc'].append(logs['val_acc'])\n",
    "        self.all['loss'].append(logs['loss']);\n",
    "        self.all['val_loss'].append(logs['val_loss']);\n",
    "            \n",
    "        clear_output();\n",
    "        #notify_slack('Finished epoch ' + str(self.num) + ' with ' + str(logs));\n",
    "        plt.plot(self.all['acc'])\n",
    "        plt.plot(self.all['val_acc'])\n",
    "\n",
    "\n",
    "        plt.title('model accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "        plt.show()\n",
    "        # summarize history for loss\n",
    "        plt.plot(self.all['loss'])\n",
    "        plt.plot(self.all['val_loss'])\n",
    "\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "def build_model(chars):\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=(None, char_len)))\n",
    "    model.add(Dense(char_len))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimizer = RMSprop(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']);\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_generate(test_x_1, test_x_2, seed_1, seed_2):\n",
    "    both_labels = []\n",
    "    both_feats = []\n",
    "    \n",
    "    rands = [random.Random(), random.Random()]\n",
    "\n",
    "    rands[0].seed(seed_1)\n",
    "    rands[1].seed(sys.maxsize - seed_2)\n",
    "    \n",
    "    \n",
    "    key = 0\n",
    "    for test_x in [test_x_1, test_x_2]:\n",
    "        labels = []\n",
    "        feats = []\n",
    "        for i in range(100): \n",
    "            r1 = rands[key].randint(0, len(test_x) - 1)\n",
    "            \n",
    "            ind = test_x[r1]\n",
    "            r2 = rands[key].randint(0, len(ind) - 5)\n",
    "\n",
    "            sub_string = ind[r2:r2+5]\n",
    "\n",
    "            feats.append(sub_string)\n",
    "            labels.append(key)\n",
    "            \n",
    "        both_labels.append(labels)\n",
    "        both_feats.append(feats)\n",
    "        key = key^1\n",
    "        \n",
    "    return both_feats, both_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_samples_1 = []\n",
    "all_labels_1 = []\n",
    "\n",
    "all_samples_2 = []\n",
    "all_labels_2 = []\n",
    "\n",
    "for idx, test_x in enumerate(all_test_x):\n",
    "    for idx2, test_x2 in enumerate(all_test_x):\n",
    "        \n",
    "        [[feats_1, feats_2], [labels_1, labels_2]] = random_generate(test_x, test_x2, idx, idx2)\n",
    "        all_samples_1.append(feats_1)\n",
    "        all_labels_1.append(labels_1)\n",
    "        \n",
    "        all_samples_2.append(feats_2)\n",
    "        all_labels_2.append(labels_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "print(all_samples_1[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_on_sample(model, test_val):\n",
    "    start = np.zeros((1, 1, char_len), dtype=bool);\n",
    "    start_prob = model.predict(start);\n",
    "\n",
    "    next_vec = start.copy()[0][0];\n",
    "    probs = [];\n",
    "\n",
    "    probs.append(start_prob[0,np.argwhere(test_val[0])[0][0]]);\n",
    "\n",
    "    for idx, vec in enumerate(test_val):\n",
    "        next_vec = np.append(next_vec, vec).reshape(1, idx+2, char_len)\n",
    "        next_prob = model.predict(next_vec);\n",
    "\n",
    "        probs.append(next_prob[0, np.argwhere(test_val[idx])[0][0]]);\n",
    "        \n",
    "    return np.sum(np.log(probs));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_results(model_1, model_2, lang_idx_1, lang_idx_2):\n",
    "    \n",
    "    test_data = np.array(all_samples_1[lang_idx_1] + all_samples_2[lang_idx_2]);\n",
    "    test_labels = np.array(all_labels_1[lang_idx_1] + all_labels_2[lang_idx_2]);\n",
    "    \n",
    "    preds_1 = np.array([predict_on_sample(model_1, x) for x in test_data]);\n",
    "    preds_2 = np.array([predict_on_sample(model_2, x) for x in test_data]);\n",
    "    \n",
    "    ratio_probs = preds_1 - preds_2;\n",
    "    ratio_probs_labels = ratio_probs.copy();\n",
    "    ratio_probs_labels[ratio_probs_labels >= 1] = 1;\n",
    "    ratio_probs_labels[ratio_probs_labels < 1] = 0;\n",
    "    \n",
    "    roc = roc_auc_score(test_labels, ratio_probs_labels);\n",
    "    acc = accuracy_score(test_labels, ratio_probs_labels);\n",
    "    \n",
    "    strr = ['-Language 1' , language_names[lang_idx_1],\n",
    "            '-Language 2' , language_names[lang_idx_2],\n",
    "            '-ROC' , str(roc),\n",
    "            '-ACC' , str(acc)]\n",
    "    \n",
    "    \n",
    "    return '\\t'.join(strr), roc;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_predict(lang_idx_1, lang_idx_2, total_epochs, batch_size=2048):\n",
    "    model_1 = build_model(all_chars[lang_idx_1])\n",
    "    model_2 = build_model(all_chars[lang_idx_2]);\n",
    "    #notify_slack('--------------------------------------------------------------------------------------------------------------------------------------------');\n",
    "    epochs_ran = 0;\n",
    "    \n",
    "    history_1 = model_1.fit(all_train_x[lang_idx_1], all_train_y[lang_idx_1],\n",
    "                        batch_size=batch_size, epochs=total_epochs, shuffle=True, verbose=0);\n",
    "    history_2 = model_2.fit(all_train_x[lang_idx_2], all_train_y[lang_idx_2],\n",
    "                        batch_size=batch_size, epochs=total_epochs, shuffle=True, verbose=0);\n",
    "\n",
    "    res, roc = predict_results(model_1, model_2, lang_idx_1, lang_idx_2);\n",
    "    #notify_slack(res);\n",
    "           \n",
    "    return roc, history_1, history_2, model_1, model_2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Build model...\n",
      "Finished: English -> English with ROC=0.485000\n",
      "Build model...\n",
      "Build model...\n",
      "Finished: English -> French with ROC=0.490000\n",
      "Build model...\n",
      "Build model...\n",
      "Finished: English -> Dutch with ROC=0.500000\n",
      "Build model...\n",
      "Build model...\n",
      "Finished: English -> German with ROC=0.515000\n",
      "Build model...\n",
      "Build model...\n",
      "Finished: English -> Italian with ROC=0.480000\n",
      "Build model...\n",
      "Build model...\n",
      "Finished: English -> Latin with ROC=0.480000\n",
      "Build model...\n",
      "Build model...\n",
      "Finished: English -> Portugese with ROC=0.510000\n",
      "Build model...\n",
      "Build model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-842f43e8055e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mroc_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_idx_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang_idx_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mmodel_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mroc_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-c29901217547>\u001b[0m in \u001b[0;36mtrain_and_predict\u001b[0;34m(lang_idx_1, lang_idx_2, total_epochs, batch_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m                         batch_size=batch_size, epochs=total_epochs, shuffle=True, verbose=0);\n\u001b[1;32m      9\u001b[0m     history_2 = model_2.fit(all_train_x[lang_idx_2], all_train_y[lang_idx_2],\n\u001b[0;32m---> 10\u001b[0;31m                         batch_size=batch_size, epochs=total_epochs, shuffle=True, verbose=0);\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang_idx_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang_idx_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    851\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2101\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2103\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_models = []\n",
    "\n",
    "for idx, lang in enumerate(all_text):\n",
    "    model_results = []\n",
    "    roc_results = []\n",
    "    for idx_2, lang2 in enumerate(all_text):\n",
    "        result = train_and_predict(lang_idx_1=0, lang_idx_2=0, total_epochs=8, batch_size=2512)\n",
    "        model_results.append(result)\n",
    "        roc_results.append(result[0])\n",
    "        print (\"Finished: %s -> %s with ROC=%f\" % (language_names[idx], language_names[idx_2], result[0]))\n",
    "        \n",
    "    all_models.append(model_results)\n",
    "    roc_string = ' , '.join(roc_results)\n",
    "    notify_slack(\"Finished all models for \" + language_names[idx] + ' with ' + roc_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_1 in range(len(all_models)):\n",
    "    for idx_2 in range(len(all_models[idx_1])):\n",
    "        roc, history_1, history_2, model_1, model_2 = all_models[idx_1][idx_2]\n",
    "        print (\"%s - %s ROC: %f\" % (language_names[idx_1], language_names[idx_2], roc))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plot_graphs = True;\n",
    "if plot_graphs == True:\n",
    "    plt.plot(history_english.history['acc'])\n",
    "    #plt.plot(history_english.history['val_acc'])\n",
    "\n",
    "    plt.plot(history_french.history['acc'])\n",
    "    #plt.plot(history_french.history['val_acc'])\n",
    "\n",
    "    #legend = ['eng_train', 'eng_test', 'frn_train', 'frn_test'];\n",
    "    legend = ['English', 'French'];\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(legend, loc='upper left')\n",
    "    #plt.savefig('accuracy_e_60___lr_0.001___b_64.png')\n",
    "\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history_english.history['loss'])\n",
    "    #plt.plot(history_english.history['val_loss'])\n",
    "\n",
    "    plt.plot(history_french.history['loss'])\n",
    "    #plt.plot(history_french.history['val_loss'])\n",
    "\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(legend, loc='upper left')\n",
    "    #plt.savefig('loss_e_60___lr_0.001___b_64.png')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_results(2048,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_results(2048,10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
