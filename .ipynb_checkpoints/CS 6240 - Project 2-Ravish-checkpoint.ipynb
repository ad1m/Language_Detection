{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>LSTM Language Detection</center></h1>\n",
    "<h3><center>CSE 6240 - Websearch and Text Mining</center></h3>\n",
    "\n",
    "<h7><center>Adam Lieberman, Garrett Mallory, Ravish Chawla</center></h7>\n",
    "<h7><center>April 25, 2017</center></h7>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) make 80/20 split on end and frn languages. (lower case all letters)\n",
    "2) create two lstm models (size 128)\n",
    "3) train 5 epochs, 0.2 validation split on the training samples \n",
    "4) \n",
    "4) train on 100 examples of 5 character strings from text with the right label.\n",
    "\n",
    "pass new word through lstm to get log probability. then do generative log ratio test. y = sign(pe/pj -1) to get the label. Keep scores for ROC.\n",
    "\n",
    "to get log probability, need p(t), p(r|t), p(u|tr) ... etc. Do this by passing in t, then tr then tru .... This gives you the most likely next character distribution so index into that to get the prob of \"U\" from \"tr\".\n",
    "\n",
    "pass both sets of 100 test (all 200) through both networks for the probability part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function;\n",
    "import sys;\n",
    "import random;\n",
    "import numpy as np;\n",
    "\n",
    "from keras.models import Sequential;\n",
    "from keras.layers import Dense, Activation;\n",
    "from keras.layers import LSTM;\n",
    "from keras.optimizers import RMSprop;\n",
    "from keras.utils.data_utils import get_file;\n",
    "from sklearn.cross_validation import train_test_split;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus length: 10746\n",
      "French corpus length: 12009\n"
     ]
    }
   ],
   "source": [
    "english_text = open('eng.txt').read().lower()\n",
    "french_text = open('frn.txt').read().lower()\n",
    "\n",
    "print('English corpus length:', len(english_text))\n",
    "print('French corpus length:', len(french_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_chars = sorted(list(set(english_text)))\n",
    "french_chars = sorted(list(set(french_text)))\n",
    "\n",
    "english_char_map = dict((c, i) for i, c in enumerate(english_chars))\n",
    "french_char_map = dict((c, i) for i, c in enumerate(french_chars))\n",
    "\n",
    "english_char_map_inverse = dict((i, c) for i, c in enumerate(english_chars))\n",
    "french_char_map_inverse = dict((i, c) for i, c in enumerate(french_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English character count: 43\n",
      "French character count: 41\n",
      "not present: '\n"
     ]
    }
   ],
   "source": [
    "print('English character count:', len(english_chars))\n",
    "print('French character count:', len(french_chars))\n",
    "for c in french_chars:\n",
    "    if c not in english_chars:\n",
    "        print(\"not present:\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb English sequences: 3569\n",
      "nb French sequences: 3990\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "\n",
    "english_sentences = []\n",
    "english_next_chars = []\n",
    "for i in range(0, len(english_text) - maxlen, step):\n",
    "    english_sentences.append(english_text[i: i + maxlen])\n",
    "    english_next_chars.append(english_text[i + maxlen])\n",
    "\n",
    "french_sentences = []\n",
    "french_next_chars = []\n",
    "for i in range(0, len(french_text) - maxlen, step):\n",
    "    french_sentences.append(french_text[i: i + maxlen])\n",
    "    french_next_chars.append(french_text[i + maxlen])\n",
    "    \n",
    "print('nb English sequences:', len(english_sentences))\n",
    "print('nb French sequences:', len(french_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "\n",
    "larger_len = max(len(english_chars), len(french_chars));\n",
    "\n",
    "english_x = np.zeros((len(english_sentences), maxlen, larger_len), dtype=np.bool)\n",
    "english_y = np.zeros((len(english_sentences), larger_len), dtype=np.bool)\n",
    "for i, sentence in enumerate(english_sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        english_x[i, t, english_char_map[char]] = 1\n",
    "    english_y[i, english_char_map[english_next_chars[i]]] = 1\n",
    "    \n",
    "    \n",
    "french_x = np.zeros((len(french_sentences), maxlen, larger_len), dtype=np.bool)\n",
    "french_y = np.zeros((len(french_sentences), larger_len), dtype=np.bool)\n",
    "for i, sentence in enumerate(french_sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        french_x[i, t, french_char_map[char]] = 1\n",
    "    french_y[i, french_char_map[french_next_chars[i]]] = 1\n",
    "\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train_x, english_test_x, english_train_y, english_test_y = train_test_split(english_x, english_y, test_size=0.2, random_state=1024);\n",
    "french_train_x, french_test_x, french_train_y, french_test_y = train_test_split(french_x, french_y, test_size=0.2, random_state=1024);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Shapes\n",
      "(2855, 40, 43)\n",
      "(2855, 43)\n",
      "(714, 40, 43)\n",
      "(714, 43)\n",
      "\n",
      "French Shapes\n",
      "(3192, 40, 43)\n",
      "(3192, 43)\n",
      "(798, 40, 43)\n",
      "(798, 43)\n"
     ]
    }
   ],
   "source": [
    "print('English Shapes');\n",
    "print(english_train_x.shape);\n",
    "print(english_train_y.shape);\n",
    "print(english_test_x.shape);\n",
    "print(english_test_y.shape);\n",
    "print()\n",
    "print('French Shapes');\n",
    "print(french_train_x.shape);\n",
    "print(french_train_y.shape);\n",
    "print(french_test_x.shape);\n",
    "print(french_test_y.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "def build_model(chars):\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(maxlen, larger_len)))\n",
    "    model.add(Dense(larger_len))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimizer = RMSprop(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "english_model = build_model(english_chars)\n",
    "french_model = build_model(french_chars);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3192/3192 [==============================] - 2s - loss: 2.9715     \n",
      "Epoch 2/5\n",
      "2816/3192 [=========================>....] - ETA: 0s - loss: 2.6063"
     ]
    }
   ],
   "source": [
    "history_english = english_model.fit(english_train_x, english_train_y,\n",
    "                    #batch_size=128, epochs=5);\n",
    "history_french = french_model.fit(french_train_x, french_train_y,\n",
    "                    batch_size=128, epochs=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3569, 40, 43)\n",
      "(2855, 40, 43)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Epoch 1/3\n",
      "3569/3569 [==============================] - 2s - loss: 2.1078     \n",
      "Epoch 2/3\n",
      "3569/3569 [==============================] - 2s - loss: 1.9466     \n",
      "Epoch 3/3\n",
      "3569/3569 [==============================] - 2s - loss: 1.7971     \n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ance and friendship among all nations, r\"\n",
      "ance and friendship among all nations, right the right the right the right the right the right the right the fright to ingerting the fure the right the right the right the right the right the right the right the right to beryone his the right the right the right the right the right the right the right the right the right the fumeryone his the right the right the right the right the right to suche the the right the right the right the ri\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ance and friendship among all nations, r\"\n",
      "ance and friendship among all nations, right the eduches the thas the right to and frestet of this duciugity ald shall be right th  uughe the ducligetiont enticle ad the freecengeng to eryong the right tre the right the promong the puriogetion the puight the ingersting the right to berte to right to prright the fume demders the deverting th ingte ingery cothel rights this dumand socuad frmertigst the rishty the rights and undicll pright\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ance and friendship among all nations, r\"\n",
      "ance and friendship among all nations, recght elireg isseiginy and decuudem trat erpigy of himkng entic;er agcarsety mhis this  uightecsthe, arouncigprearas enct of the duswitment. article 6rimetraty of puacirremtiget. 2. prufotatits ofs. mffryote ofy acl nd riightr ngsepuan, sravatitionalion thiculinationy ingece yaalusen. formothel cimm\n",
      "d. thalinalions and fre mes the eraceinelresasy acticlt obitho duoit. 1 the puriinane ind thefryunt\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ance and friendship among all nations, r\"\n",
      "ance and friendship among all nations, r.othd hustond inclerathegsty. eicar cuothtorid predenges catlrexfond alt ict orsstad the redecce ots sfaf hipli3. to fualayecufice stmoryac. ard yuclumintee\f",
      "dus dussempxrtit. o, timtcentey frighty. duvicy, dumdumstrggand y mongt ane gedpldary incleplof arnicothed bo bhis- is. aghimcl sbety, adtey artigationstionarion, iatunstnst xy husuwy thesparan onllsissecs tf fomed suaces. botoestingiseditmond\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 1 input samples and 3569 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-fcf55ea844ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish_chars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish_char_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish_char_map_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-ceaaefe7780a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, text, chars, char_map, char_map_inverse, x, y)\u001b[0m\n\u001b[1;32m      8\u001b[0m         model.fit(x, y,\n\u001b[1;32m      9\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                   epochs=3)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mstart_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    851\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1404\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1407\u001b[0m         \u001b[0;31m# prepare validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1306\u001b[0m                           \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m                           in zip(y, sample_weights, class_weights, self._feed_sample_weight_modes)]\n\u001b[0;32m-> 1308\u001b[0;31m         \u001b[0m_check_array_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m         _check_loss_and_target_compatibility(y,\n\u001b[1;32m   1310\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_loss_fns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_check_array_lengths\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    227\u001b[0m                          \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                          \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mset_y\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mset_w\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         raise ValueError('Sample_weight arrays should have '\n",
      "\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 1 input samples and 3569 target samples."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
