<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Part 2</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="css/normalize.css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="css/cayman.css">
    <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
      });
    </script>
  </head>
  <body>

    <style>
    #right_div {
    float: right;
    border: 0px solid black;
    text-align: center;
}
</style>


    <section class="page-header">
      <br>
      <br>
      <h1 class="project-name">Long Short-Term Memory</h1>
      <h2 class="project-tagline">Part 2: Building an LSTM for Language Detection</h2>
      <h3 class="project-tagline">By: Adam Lieberman, Ravish Chawla, & Garrett Mallory</h3>
      <br>
      <br>
      <!--<a href="#" class="btn">View on GitHub</a>
      <a href="#" class="btn">Download .zip</a>
      <a href="#" class="btn">Download .tar.gz</a> -->
    </section>

    <section class="main-content">
    <h1>Introduction</h1>
    <p>If you are a frequent user of Google, you have probably seen this box before:</p>
    <center><img src="../images/google_translate.png"></center>
    <p>This is google translate. When you start typing in a sentence to translate, it automatically picks up the current language you are typing in. Let's test it out:</p>
    <center><img src="../images/google_translate_2.png"></center>
    <p>We see that we have typed in "bonjour", which means hello in french, and that google has automatically detected that our word is french. Here, Google uses recurrent neural networks to detect the current langauge and then translate it into the user's language of choice.</p>
    <p>Language detection can be accomplished through Long Short-Term Memory. Let us take on the task of langauge detection using two datasets: a dataset consiting of english sentences and a dataset consitsting of french sentences.</p>
    <!-- Explain why we want to do facial generation and current ways facial generations is done-->
    <!--<p>Radford's paper <a href="https://arxiv.org/pdf/1511.06434.pdf">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a> lays the framework and architecture for building a deep convolutional generative adversarial network (DCGAN). Convolutional layers have been great for image based deep learning regarding tasks like image classification in the field of computer vision. Additionally, DCGAN models can improve stability during training so that we hopefully do not encounter issues like mode collapse. Our goal is to build a DCGAN that can generate synthetic facial images. We will follow the steps Radford used in his paper to build out our model.</p> -->

    <h1>Environment Setup</h1>
    <p>Below we have a description of each library we will use. Please click the links under installation and documentation to install and learn more about each library.</p>
    <ul>
    <li><b>Programming Language: </b></li>
    <br>
    <ul>
    <li>Python 3 - Python is a general-purpose interpreted, interactive, object-oriented, and high-level programming language. We will be using version 3.x. This can be obtained from the official Python website or through the Anaconda distribution, which contains python 3 and many useful scientific computing libraries.</li>
    <ul>
    <li>Python Installation: <a herf="https://www.python.org/download/releases/3.0/">Python Installation</a> or <a href="https://www.continuum.io/downloads">Anaconda Installation</a></li>
    <li>Pip Installation: <a href="https://pip.pypa.io/en/stable/installing/">Pip Installation</a></li>
    </ul>
    </ul>
    <br>
    <li><b> Libraries: </b></li>
    <br>
    <ul>
    <li>TensorFlow - TensorFlow is an open-source software for machine intelligence. It is currently a very popular choice for developing deep learning modles.</li>
    <ul>
    <li>Installation: <a href="https://www.tensorflow.org/install/">TensorFlow Installation</a></li>
    <li>Documentation: <a href="https://www.tensorflow.org/get_started/get_started">TensorFlow Documentation</a></li>
    </ul>
    <br>
    <li>Numpy - Numpy is a package for scientific computing that contains many useful operations for a multi-dimensional data structure called an ndarray (np array).</li>
    <ul>
    <li>Installation: <a href="https://www.scipy.org/scipylib/download.html">Numpy Installation</a></li>
    <li>Documentation: <a href="https://docs.scipy.org/doc/numpy-dev/user/basics.html">Numpy Documentation</a></li>
    </ul>
    <br>
    <li>Matplotlib -  Matplotlib, from the creators of numpy, is a plotting library that allows for custom charts like scatter plots, bar charts, line graphs, etc.</li>
    <ul>
    <li>Installation: <a href="http://matplotlib.org/users/installing.html">Matplotlib Installation</a></li>
    <li>Documentation: <a href="http://matplotlib.org/2.0.0/examples/index.html">Matplotlib Documentation</a></li>
    </ul>
    <br>
    <li>Seaborn -  Seaborn is an alternate plotting library that allows for easy construction of plots such as heatmaps.</li>
    <ul>
    <li>Installation: <a href="https://seaborn.pydata.org/installing.html">Matplotlib Installation</a></li>
    <li>Documentation: <a href="https://seaborn.pydata.org/tutorial.html">Matplotlib Documentation</a></li>
    </ul>
    <br>
    <li>Scipy - Scipy, from the creators of numpy, is an alternate library for mathematics, science, and engineering.</li>
    <ul>
    <li>Installation: <a href="https://www.scipy.org">Scipy Installation</a></li>
    <li>Documentation: <a href="https://docs.scipy.org/doc/scipy/reference/">Scipy Documentation</a></li>
    </ul>
    <br>
    <li>Sklearn - Sklearn has efficient tools for data mining, analysis, and machine learning.</li>
    <ul>
    <li>Installation: <a href="http://scikit-learn.org/stable/install.html">Sklearn Installation</a></li>
    <li>Documentation: <a href="http://scikit-learn.org/stable/documentation.html">Sklearn Documentation</a></li>
    </ul>
    <!--<li>PIL - PIL, short for Python Imaging Library, is a powerful library to handle and manipulate images.</li>
    <ul>
    <li>Installation: <a href="https://pillow.readthedocs.io/en/4.0.x/installation.html">PIL Installation</a></li>
    <li>Documentation: <a href="https://pillow.readthedocs.io/en/4.0.x/handbook/index.html">PIL Documentation</a></li>
    </ul> -->
    <br>

    <li>Random - Random allows us to create pseudo-random number generators for various distributions.</li>
    <ul>
    <li>Installation: Installed with Python</li>
    <li>Documentation: <a href="https://docs.python.org/2/library/random.html">Random Documentation</a></li>
    </ul>
    </ul>
    </ul>

    <h1>Data</h1>
    <p>We will be detecting language between English and French. For this task, we will use the datasets found <a href="https://github.com/GT-CSE6240/proj2/tree/master/data">here</a>. At this link there will be eng.txt and frn.txt. Both datasets contain sentences, in their respective language, from the Universal Declaration of Human Rights.</p>

    <h1>Imports</h1>
    <p>We start by importing the following libraries:</p>
    <!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">__future__</span> <span style="color: #0000aa">import</span> print_function;
<span style="color: #0000aa">import</span> <span style="color: #00aaaa; text-decoration: underline">sys</span>;
<span style="color: #0000aa">import</span> <span style="color: #00aaaa; text-decoration: underline">random</span>;
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">random</span> <span style="color: #0000aa">import</span> randint

<span style="color: #0000aa">import</span> <span style="color: #00aaaa; text-decoration: underline">numpy</span> <span style="color: #0000aa">as</span> <span style="color: #00aaaa; text-decoration: underline">np</span>;

<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">keras.models</span> <span style="color: #0000aa">import</span> Sequential;
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">keras.layers</span> <span style="color: #0000aa">import</span> Dense, Activation;
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">keras.layers</span> <span style="color: #0000aa">import</span> LSTM;
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">keras.optimizers</span> <span style="color: #0000aa">import</span> RMSprop;
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">keras.utils.data_utils</span> <span style="color: #0000aa">import</span> get_file;
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">keras.models</span> <span style="color: #0000aa">import</span> load_model;

<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">sklearn.cross_validation</span> <span style="color: #0000aa">import</span> train_test_split;
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">sklearn.metrics</span> <span style="color: #0000aa">import</span> *;
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">sklearn.externals</span> <span style="color: #0000aa">import</span> joblib;

<span style="color: #0000aa">import</span> <span style="color: #00aaaa; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #0000aa">as</span> <span style="color: #00aaaa; text-decoration: underline">plt</span>;
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">IPython.display</span> <span style="color: #0000aa">import</span> clear_output
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">keras.callbacks</span> <span style="color: #0000aa">import</span> ModelCheckpoint, Callback

<span style="color: #0000aa">import</span> <span style="color: #00aaaa; text-decoration: underline">subprocess</span>;
<span style="color: #0000aa">import</span> <span style="color: #00aaaa; text-decoration: underline">h5py</span>;
</pre></div>



    <h1>Loading Data</h1>
    <p>Now that we have our English and French data, we need to load it. To do so we can open the filepath of the data and then use .read() to read it in. Additionally, we lower case the alphabet in the respective datafiles. Additionally, we will print the length of our English and French corpus so that we can see how much data we are dealing with. We do so as follows:</p>
    <!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">english_text = <span style="color: #00aaaa">open</span>(<span style="color: #aa5500">&#39;data/eng.txt&#39;</span>).read().lower()
french_text = <span style="color: #00aaaa">open</span>(<span style="color: #aa5500">&#39;data/frn.txt&#39;</span>).read().lower()

<span style="color: #0000aa">print</span>(<span style="color: #aa5500">&#39;English corpus length:&#39;</span>, <span style="color: #00aaaa">len</span>(english_text))
<span style="color: #0000aa">print</span>(<span style="color: #aa5500">&#39;French corpus length:&#39;</span>, <span style="color: #00aaaa">len</span>(french_text))

&gt;&gt;&gt;
English corpus length: <span style="color: #009999">10746</span>
French corpus length: <span style="color: #009999">12009</span>
</pre></div>
    <p>We see that our Engligh corpus has length 10746 and our French corpus has length 12009.</p>

    <h1>Character Set and Dictionary</h1>
    <p>Now that we have our English and French text loaded in we will need to obtain the character map for both datasets. To do so we can first create a set of the characters and then iterate over each character to create a dictionary where the keys are the characters and the values are integers from 0 to len(characters). Additionally, we create an inverse dictionary where the keys and values are flipped. The purpose of creating these dictionaries are to map the features to the feature indices in our upcoming design matrix. We do so as follows:</p>

    <!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">english_chars = <span style="color: #00aaaa">sorted</span>(<span style="color: #00aaaa">list</span>(<span style="color: #00aaaa">set</span>(english_text)))
french_chars = <span style="color: #00aaaa">sorted</span>(<span style="color: #00aaaa">list</span>(<span style="color: #00aaaa">set</span>(french_text)))

english_char_map = <span style="color: #00aaaa">dict</span>((c, i) <span style="color: #0000aa">for</span> i, c <span style="color: #0000aa">in</span> <span style="color: #00aaaa">enumerate</span>(english_chars))
french_char_map = <span style="color: #00aaaa">dict</span>((c, i) <span style="color: #0000aa">for</span> i, c <span style="color: #0000aa">in</span> <span style="color: #00aaaa">enumerate</span>(french_chars))

english_char_map_inverse = <span style="color: #00aaaa">dict</span>((i, c) <span style="color: #0000aa">for</span> i, c <span style="color: #0000aa">in</span> <span style="color: #00aaaa">enumerate</span>(english_chars))
french_char_map_inverse = <span style="color: #00aaaa">dict</span>((i, c) <span style="color: #0000aa">for</span> i, c <span style="color: #0000aa">in</span> <span style="color: #00aaaa">enumerate</span>(french_chars))
</pre></div>

<h1>Sentence Creation</h1>
<p>We now create sentences from both the English and French text, with a rolling window of 40 characters. These will serve as our features. To do so we set a maxlength of 40 and a step size of 3. We create an empty list for the sentences and an empty list for the next charaters. We then iterate over the length of the text minus the maxlength with a step size of 3 and append the text of the current character to 40 characters from that character. We also append the next character (character i + 40) to the next characters list. After doing so we will obtain an [x,40] matrix where each row will have length alphabet-size. We will additionally print the number of English and French sentences that we construct. We do so as follows:</p> 
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">maxlen = <span style="color: #009999">40</span>
step = <span style="color: #009999">3</span>

english_sentences = []
english_next_chars = []
<span style="color: #0000aa">for</span> i <span style="color: #0000aa">in</span> <span style="color: #00aaaa">range</span>(<span style="color: #009999">0</span>, <span style="color: #00aaaa">len</span>(english_text) - maxlen, step):
    english_sentences.append(english_text[i: i + maxlen])
    english_next_chars.append(english_text[i + maxlen])

french_sentences = []
french_next_chars = []
<span style="color: #0000aa">for</span> i <span style="color: #0000aa">in</span> <span style="color: #00aaaa">range</span>(<span style="color: #009999">0</span>, <span style="color: #00aaaa">len</span>(french_text) - maxlen, step):
    french_sentences.append(french_text[i: i + maxlen])
    french_next_chars.append(french_text[i + maxlen])
    
<span style="color: #0000aa">print</span>(<span style="color: #aa5500">&#39;nb English sequences:&#39;</span>, <span style="color: #00aaaa">len</span>(english_sentences))
<span style="color: #0000aa">print</span>(<span style="color: #aa5500">&#39;nb French sequences:&#39;</span>, <span style="color: #00aaaa">len</span>(french_sentences))

&gt;&gt;&gt; 
nb English sequences: <span style="color: #009999">3569</span>
nb French sequences: <span style="color: #009999">3990</span>
</pre></div>

<p>We see that we have created 3569 English sentences and 3990 French sentences.</p>
<h1>Design Matrix Creation:</h1>
<p>We can now vectorize the matrices we created above. Here, we create a feature matrix x, which is full of zeros and has size len(sentences),maxlength, character length. We will then let this matrix be of dtype boolean. We create a labels matrix y, full of zeros with size len(sentences), character length. This matrix is also of dtype boolean. We then iterate over the sentences and look at the t, char in the sentence. We set x[i,t,char_map[char]] = 1 in the inner loop and set y[i,char_map[next_chars[i]]] = 1. This will give us our vectorized features. We do so as follows:</p>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">char_len = <span style="color: #00aaaa">max</span>(<span style="color: #00aaaa">len</span>(english_chars), <span style="color: #00aaaa">len</span>(french_chars));

english_x = np.zeros((<span style="color: #00aaaa">len</span>(english_sentences), maxlen, char_len), dtype=np.bool)
english_y = np.zeros((<span style="color: #00aaaa">len</span>(english_sentences), char_len), dtype=np.bool)
<span style="color: #0000aa">for</span> i, sentence <span style="color: #0000aa">in</span> <span style="color: #00aaaa">enumerate</span>(english_sentences):
    <span style="color: #0000aa">for</span> t, char <span style="color: #0000aa">in</span> <span style="color: #00aaaa">enumerate</span>(sentence):
        english_x[i, t, english_char_map[char]] = <span style="color: #009999">1</span>
    english_y[i, english_char_map[english_next_chars[i]]] = <span style="color: #009999">1</span>
    
    
french_x = np.zeros((<span style="color: #00aaaa">len</span>(french_sentences), maxlen, char_len), dtype=np.bool)
french_y = np.zeros((<span style="color: #00aaaa">len</span>(french_sentences), char_len), dtype=np.bool)
<span style="color: #0000aa">for</span> i, sentence <span style="color: #0000aa">in</span> <span style="color: #00aaaa">enumerate</span>(french_sentences):
    <span style="color: #0000aa">for</span> t, char <span style="color: #0000aa">in</span> <span style="color: #00aaaa">enumerate</span>(sentence):
        french_x[i, t, french_char_map[char]] = <span style="color: #009999">1</span>
    french_y[i, french_char_map[french_next_chars[i]]] = <span style="color: #009999">1</span>
</pre></div>

<h1>Train/Test Split:</h1>
<p>Now that we have our Englisn and French features with their corresponding labels, we can use sklearn's train_test_split function to obtain a split of 80% training data and 20% testing data. Here, we will set a random_state = 1024. We do so as follows:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">english_train_x, english_test_x, english_train_y, english_test_y = train_test_split(english_x, english_y, test_size=<span style="color: #009999">0.2</span>, random_state=<span style="color: #009999">1024</span>);
french_train_x, french_test_x, french_train_y, french_test_y = train_test_split(french_x, french_y, test_size=<span style="color: #009999">0.2</span>, random_state=<span style="color: #009999">1024</span>);
</pre></div>
<p>Now that we have our training and testing data, let us look at the shapes of the matrices.</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">print</span>(<span style="color: #aa5500">&#39;English Shapes&#39;</span>);
<span style="color: #0000aa">print</span>(english_train_x.shape);
<span style="color: #0000aa">print</span>(english_train_y.shape);
<span style="color: #0000aa">print</span>(english_test_x.shape);
<span style="color: #0000aa">print</span>(english_test_y.shape);
<span style="color: #0000aa">print</span>()
<span style="color: #0000aa">print</span>(<span style="color: #aa5500">&#39;French Shapes&#39;</span>);
<span style="color: #0000aa">print</span>(french_train_x.shape);
<span style="color: #0000aa">print</span>(french_train_y.shape);
<span style="color: #0000aa">print</span>(french_test_x.shape);
<span style="color: #0000aa">print</span>(french_test_y.shape);

&gt;&gt;&gt;
English Shapes
(<span style="color: #009999">2855</span>, <span style="color: #009999">40</span>, <span style="color: #009999">43</span>)
(<span style="color: #009999">2855</span>, <span style="color: #009999">43</span>)
(<span style="color: #009999">714</span>, <span style="color: #009999">40</span>, <span style="color: #009999">43</span>)
(<span style="color: #009999">714</span>, <span style="color: #009999">43</span>)

French Shapes
(<span style="color: #009999">3192</span>, <span style="color: #009999">40</span>, <span style="color: #009999">43</span>)
(<span style="color: #009999">3192</span>, <span style="color: #009999">43</span>)
(<span style="color: #009999">798</span>, <span style="color: #009999">40</span>, <span style="color: #009999">43</span>)
(<span style="color: #009999">798</span>, <span style="color: #009999">43</span>)
</pre></div>

<p>Above, we see that both languages have [x, 40, 43] values for training, even though French has 41 total alphabet charachters. We see this because we pad the smaller language with 2 extra columns of all 0s, in order to make sure that our neural network can train properly on the same input shape.</p>

<h1>Generating Test Data:</h1>
<p>The test split above was our holdout data. For the English and French holdout sets we will generate a new set of test data by randomly selecting 100 5-character substrings from each respective dataset. Here, we will end up with 200 strings, which will be our test features for evaluation. Our labels for the corresponding test data will have a 1 for English and a 0 for French. We will have 100 1's in the test labels and 100 0's. To create this test data we will create a function called random_generate(), which takes in our test data and a key. We start by creating two empty lists called labels, which will hold our labels, and feats, which will hold our features. We have a conditional that check if the key is "english" or "french". If the key is english then the labels vector will contain 100 1's. If it is french than the labels vector will contain 100 0's. We now iterate over range(100) and generate a random number between 0 and the length of the test data passed in. This will represent a random index to pull from inside the test data. We then generate another random integer between 0 and the length of the data in the index minus 5. This will represent a starting character. We can then extract this character plus the next 4 characters to obtain our random 5 character substring. We append this into the features variable. We will call this process for the English data and the French data and then combine the lists of the features and the lists of the labels and make each one a numpy array. We do so as follows:</p>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">def</span> <span style="color: #00aa00">random_generate</span>(test_x, key):
    labels = []
    feats = []
    <span style="color: #0000aa">if</span> key == <span style="color: #aa5500">&quot;english&quot;</span>: 
        labels = [<span style="color: #009999">1</span> <span style="color: #0000aa">for</span> i <span style="color: #0000aa">in</span> <span style="color: #00aaaa">range</span>(<span style="color: #009999">100</span>)]
    <span style="color: #0000aa">elif</span> key == <span style="color: #aa5500">&#39;french&#39;</span>: 
        labels = [<span style="color: #009999">0</span> <span style="color: #0000aa">for</span> i <span style="color: #0000aa">in</span> <span style="color: #00aaaa">range</span>(<span style="color: #009999">100</span>)]
    <span style="color: #0000aa">else</span>:
        <span style="color: #0000aa">return</span> feats, labels;
    
    <span style="color: #0000aa">for</span> i <span style="color: #0000aa">in</span> <span style="color: #00aaaa">range</span>(<span style="color: #009999">100</span>): 
        r1 = randint(<span style="color: #009999">0</span>, <span style="color: #00aaaa">len</span>(test_x) - <span style="color: #009999">1</span>)
        ind = test_x[r1]
        
        r2 = randint(<span style="color: #009999">0</span>, <span style="color: #00aaaa">len</span>(ind) - <span style="color: #009999">5</span>)
        
        sub_string = ind[r2:r2+<span style="color: #009999">5</span>]
        
        feats.append(sub_string)
        
    <span style="color: #0000aa">return</span> feats,labels
    
english_sample, english_labels = random_generate(english_test_x, <span style="color: #aa5500">&#39;english&#39;</span>)
french_sample, french_labels = random_generate(french_test_x, <span style="color: #aa5500">&#39;french&#39;</span>)

test_data = np.array(english_sample + french_sample)
test_labels = np.array(english_labels + french_labels)
</pre></div>

<h1>LSTM Model Creation:</h1>
<p>Now that we have our training and test data, we can create our Long Short-Term Memory Model. To do so we will construct a function called build_model. This will set up a Sequential model with an LSTM layer that has 256 neurons followed by a dense layer that has shape number of characters (43) and a softmax activation function. Here we will use an RMSprop optimizer, with a leanring rate of 0.01, and a loss function of categorical_crossentropy. We create our function as follows:</p>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">def</span> <span style="color: #00aa00">build_model</span>(chars):
    <span style="color: #0000aa">print</span>(<span style="color: #aa5500">&#39;Build model...&#39;</span>)
    model = Sequential()
    model.add(LSTM(<span style="color: #009999">256</span>, input_shape=(<span style="color: #00aaaa">None</span>, char_len)))
    model.add(Dense(char_len))
    model.add(Activation(<span style="color: #aa5500">&#39;softmax&#39;</span>))
    
    optimizer = RMSprop(lr=<span style="color: #009999">0.01</span>)
    model.compile(loss=<span style="color: #aa5500">&#39;categorical_crossentropy&#39;</span>, optimizer=optimizer, metrics=[<span style="color: #aa5500">&#39;accuracy&#39;</span>])
    <span style="color: #0000aa">return</span> model
</pre></div>

<h1>Prediction & Log Likelihood:</h1>
<p>For each test string, we need to compute the log likelihood of that string for each model. To do so we will create a function called predict_on_sample which takes in a model and a test value. Here, we will iterate over each charachter, obtain the vector associated with it along with the previous START charachters, and obtain the prediction score for it on the next charachter. Finally, we sum up the logs of these values and return it. We create this function as follows:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">def</span> <span style="color: #00aa00">predict_on_sample</span>(model, test_val):
    start = np.zeros((<span style="color: #009999">1</span>, <span style="color: #009999">1</span>, char_len), dtype=<span style="color: #00aaaa">bool</span>)
    start_prob = model.predict(start)

    next_vec = start.copy()[<span style="color: #009999">0</span>][<span style="color: #009999">0</span>]
    probs = []

    probs.append(start_prob[<span style="color: #009999">0</span>,np.argwhere(test_val[<span style="color: #009999">0</span>])[<span style="color: #009999">0</span>][<span style="color: #009999">0</span>]])

    <span style="color: #0000aa">for</span> idx, vec <span style="color: #0000aa">in</span> <span style="color: #00aaaa">enumerate</span>(test_val):
        next_vec = np.append(next_vec, vec).reshape(<span style="color: #009999">1</span>, idx+<span style="color: #009999">2</span>, char_len)
        next_prob = model.predict(next_vec)

        probs.append(next_prob[<span style="color: #009999">0</span>, np.argwhere(test_val[idx])[<span style="color: #009999">0</span>][<span style="color: #009999">0</span>]])
        
    <span style="color: #0000aa">return</span> np.sum(np.log(probs))
</pre></div>
    <h1>ROC</h1>
    <p>Using the above function, we can predict the results on each test string. We can then create a function called predict_results(). We pass in the string along with both the English model and the French model, obtain the predicted probabilities, and compute the Generalized Likelihood Ratio Test (GLRT). The ratio gives us a vector of probability scores, in which we set all values >= 1 to 1, and otherwise 0. We can then obtain the false positive rate and true positive rate from the roc_curve function in the sklearn library and then use those to get the roc_auc from the auc function in sklearn. We create the function as follows:</p>

    <!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">def</span> <span style="color: #00aa00">predict_results</span>(english_model, french_model):
    english_preds = np.array([predict_on_sample(english_model, x) <span style="color: #0000aa">for</span> x <span style="color: #0000aa">in</span> test_data])
    french_preds = np.array([predict_on_sample(french_model, x) <span style="color: #0000aa">for</span> x <span style="color: #0000aa">in</span> test_data])
    ratio_probs = english_preds - french_preds
        
    fpr, tpr, _ = roc_curve(test_labels, ratio_probs);
    roc_auc = auc(fpr, tpr)
    
    <span style="color: #0000aa">print</span>(roc_auc)

    <span style="color: #0000aa">return</span> roc_auc, fpr, tpr
</pre></div>
<p>Now that we have the ROC, we can create a function called plot_roc_auc_curve to plot our ROC curve, which takes in the false positive rate, the true positive rate, the roc_auc, and a title. We create this function as follows:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">def</span> <span style="color: #00aa00">plot_roc_auc_curve</span>(fpr, tpr, roc_auc, title): 
    plt.figure()
    lw = <span style="color: #009999">2</span>
    plt.plot(fpr, tpr, color=<span style="color: #aa5500">&#39;darkorange&#39;</span>,
             lw=lw, label=<span style="color: #aa5500">&#39;ROC curve (area = %0.2f)&#39;</span> % roc_auc)
    plt.plot([<span style="color: #009999">0</span>, <span style="color: #009999">1</span>], [<span style="color: #009999">0</span>, <span style="color: #009999">1</span>], color=<span style="color: #aa5500">&#39;navy&#39;</span>, lw=lw, linestyle=<span style="color: #aa5500">&#39;--&#39;</span>)
    plt.xlim([<span style="color: #009999">0.0</span>, <span style="color: #009999">1.0</span>])
    plt.ylim([<span style="color: #009999">0.0</span>, <span style="color: #009999">1.05</span>])
    plt.xlabel(<span style="color: #aa5500">&#39;False Positive Rate&#39;</span>)
    plt.ylabel(<span style="color: #aa5500">&#39;True Positive Rate&#39;</span>)
    plt.title(<span style="color: #aa5500">&#39;Receiver operating characteristic for &#39;</span> + title)
    plt.legend(loc=<span style="color: #aa5500">&quot;lower right&quot;</span>)
    plt.show()
</pre></div>

<h1>Hyperparameter Tuning:</h1>
<p>Above we have set up a functions to create our model LSTM model, obtain the predictions, compute the log likelihood, and compute the ROC. We have not instantiated the model yet as we do not know the optimal hyperparameters. Let us now perform a hyperparameter search. Previously, we wrote a function called train_and_predict(). We cycle though a set range of epochs and evaluate the model quality as the iterations increase. We then take the best performing model.
Other hyperparameters tuned are the learning rate, decay rate, batch sizes used in training, and size of the LSTM layer. Some of these we tuned manually rather than create a automated function for it. Finally, we found that the best performance is given by epochs=8, learning rate=0.01, decay=0.0, layer_size=256.</p>
<p>Below we display graphs showing the loss vs. epochs and accuracy vs. epochs for learning rate 0.01 as a demonstration of our parameter selection process.</p>
<center><img src="http://i.imgur.com/7QhIXwl.jpg"></center>
<center><img src="http://i.imgur.com/XyPN1xj.jpg"></center>

<h1>Building English & French LSTM Models</h1>
<p>Let us create our English and French LSTM models with 5 epochs, a step size of 5, and an optimal batch size of 2512:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">models_1 = train_and_predict(<span style="color: #009999">5</span>, <span style="color: #009999">5</span>, <span style="color: #009999">2512</span>)

&gt;&gt;&gt;
Build model...
Build model...
Epoch <span style="color: #009999">1</span>/<span style="color: #009999">5</span>
<span style="color: #009999">2855</span>/<span style="color: #009999">2855</span> [==============================] - <span style="color: #009999">3</span>s - loss: <span style="color: #009999">3.7206</span> - acc: <span style="color: #009999">0.0221</span>     
Epoch <span style="color: #009999">2</span>/<span style="color: #009999">5</span>
<span style="color: #009999">2855</span>/<span style="color: #009999">2855</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">5.3884</span> - acc: <span style="color: #009999">0.1426</span>     
Epoch <span style="color: #009999">3</span>/<span style="color: #009999">5</span>
<span style="color: #009999">2855</span>/<span style="color: #009999">2855</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">4.2558</span> - acc: <span style="color: #009999">0.0676</span>     
Epoch <span style="color: #009999">4</span>/<span style="color: #009999">5</span>
<span style="color: #009999">2855</span>/<span style="color: #009999">2855</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">3.5550</span> - acc: <span style="color: #009999">0.1625</span>     
Epoch <span style="color: #009999">5</span>/<span style="color: #009999">5</span>
<span style="color: #009999">2855</span>/<span style="color: #009999">2855</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">3.4811</span> - acc: <span style="color: #009999">0.0550</span>     
Epoch <span style="color: #009999">1</span>/<span style="color: #009999">5</span>
<span style="color: #009999">3192</span>/<span style="color: #009999">3192</span> [==============================] - <span style="color: #009999">3</span>s - loss: <span style="color: #009999">3.8357</span> - acc: <span style="color: #009999">0.0711</span>     
Epoch <span style="color: #009999">2</span>/<span style="color: #009999">5</span>
<span style="color: #009999">3192</span>/<span style="color: #009999">3192</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">3.4972</span> - acc: <span style="color: #009999">0.0655</span>     
Epoch <span style="color: #009999">3</span>/<span style="color: #009999">5</span>
<span style="color: #009999">3192</span>/<span style="color: #009999">3192</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">4.1910</span> - acc: <span style="color: #009999">0.1103</span>     
Epoch <span style="color: #009999">4</span>/<span style="color: #009999">5</span>
<span style="color: #009999">3192</span>/<span style="color: #009999">3192</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">3.0199</span> - acc: <span style="color: #009999">0.0840</span>     
Epoch <span style="color: #009999">5</span>/<span style="color: #009999">5</span>
<span style="color: #009999">3192</span>/<span style="color: #009999">3192</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">2.9137</span> - acc: <span style="color: #009999">0.1732</span>     
<span style="color: #009999">0.7948</span>
</pre></div>
<p>We see that this configuration has given us an ROC of 0.7948.</p>
<p>Let us now create our LSTM models with 12 epochs, a step size of 3 epochs, and a batch size of 2512:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">models_2 = train_and_predict(<span style="color: #009999">12</span>, <span style="color: #009999">3</span>, <span style="color: #009999">2512</span>)

&gt;&gt;&gt;
Build model...
Build model...
Epoch <span style="color: #009999">1</span>/<span style="color: #009999">3</span>
<span style="color: #009999">2855</span>/<span style="color: #009999">2855</span> [==============================] - <span style="color: #009999">3</span>s - loss: <span style="color: #009999">3.8244</span> - acc: <span style="color: #009999">0.0319</span>     
Epoch <span style="color: #009999">2</span>/<span style="color: #009999">3</span>
<span style="color: #009999">2855</span>/<span style="color: #009999">2855</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">3.8091</span> - acc: <span style="color: #009999">0.0501</span>     
Epoch <span style="color: #009999">3</span>/<span style="color: #009999">3</span>
<span style="color: #009999">2855</span>/<span style="color: #009999">2855</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">4.0902</span> - acc: <span style="color: #009999">0.0630</span>     
Epoch <span style="color: #009999">1</span>/<span style="color: #009999">3</span>
<span style="color: #009999">3192</span>/<span style="color: #009999">3192</span> [==============================] - <span style="color: #009999">4</span>s - loss: <span style="color: #009999">3.6929</span> - acc: <span style="color: #009999">0.0592</span>     
Epoch <span style="color: #009999">2</span>/<span style="color: #009999">3</span>
<span style="color: #009999">3192</span>/<span style="color: #009999">3192</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">4.6424</span> - acc: <span style="color: #009999">0.1419</span>     
Epoch <span style="color: #009999">3</span>/<span style="color: #009999">3</span>
<span style="color: #009999">3192</span>/<span style="color: #009999">3192</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">4.5772</span> - acc: <span style="color: #009999">0.0617</span>     
<span style="color: #009999">0.8456</span>
Epoch <span style="color: #009999">1</span>/<span style="color: #009999">3</span>
<span style="color: #009999">2855</span>/<span style="color: #009999">2855</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">3.1018</span> - acc: <span style="color: #009999">0.1625</span>     
Epoch <span style="color: #009999">2</span>/<span style="color: #009999">3</span>
<span style="color: #009999">2855</span>/<span style="color: #009999">2855</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">3.0159</span> - acc: <span style="color: #009999">0.1541</span>     
Epoch <span style="color: #009999">3</span>/<span style="color: #009999">3</span>
<span style="color: #009999">2855</span>/<span style="color: #009999">2855</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">3.0308</span> - acc: <span style="color: #009999">0.1625</span>     
Epoch <span style="color: #009999">1</span>/<span style="color: #009999">3</span>
<span style="color: #009999">3192</span>/<span style="color: #009999">3192</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">3.7905</span> - acc: <span style="color: #009999">0.1604</span>     
Epoch <span style="color: #009999">2</span>/<span style="color: #009999">3</span>
<span style="color: #009999">3192</span>/<span style="color: #009999">3192</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">3.7677</span> - acc: <span style="color: #009999">0.1554</span>     
Epoch <span style="color: #009999">3</span>/<span style="color: #009999">3</span>
<span style="color: #009999">3192</span>/<span style="color: #009999">3192</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">3.7589</span> - acc: <span style="color: #009999">0.1560</span>     
<span style="color: #009999">0.8479</span>
Epoch <span style="color: #009999">1</span>/<span style="color: #009999">3</span>
<span style="color: #009999">2855</span>/<span style="color: #009999">2855</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">3.0133</span> - acc: <span style="color: #009999">0.1625</span>     
Epoch <span style="color: #009999">2</span>/<span style="color: #009999">3</span>
<span style="color: #009999">2855</span>/<span style="color: #009999">2855</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">2.9974</span> - acc: <span style="color: #009999">0.1604</span>     
Epoch <span style="color: #009999">3</span>/<span style="color: #009999">3</span>
<span style="color: #009999">2855</span>/<span style="color: #009999">2855</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">2.9674</span> - acc: <span style="color: #009999">0.1625</span>     
Epoch <span style="color: #009999">1</span>/<span style="color: #009999">3</span>
<span style="color: #009999">3192</span>/<span style="color: #009999">3192</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">3.6747</span> - acc: <span style="color: #009999">0.1538</span>     
Epoch <span style="color: #009999">2</span>/<span style="color: #009999">3</span>
<span style="color: #009999">3192</span>/<span style="color: #009999">3192</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">3.7156</span> - acc: <span style="color: #009999">0.1441</span>     
Epoch <span style="color: #009999">3</span>/<span style="color: #009999">3</span>
<span style="color: #009999">3192</span>/<span style="color: #009999">3192</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">3.7864</span> - acc: <span style="color: #009999">0.1444</span>     
<span style="color: #009999">0.865</span>
Epoch <span style="color: #009999">1</span>/<span style="color: #009999">3</span>
<span style="color: #009999">2855</span>/<span style="color: #009999">2855</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">2.9449</span> - acc: <span style="color: #009999">0.1653</span>     
Epoch <span style="color: #009999">2</span>/<span style="color: #009999">3</span>
<span style="color: #009999">2855</span>/<span style="color: #009999">2855</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">2.9600</span> - acc: <span style="color: #009999">0.1520</span>     
Epoch <span style="color: #009999">3</span>/<span style="color: #009999">3</span>
<span style="color: #009999">2855</span>/<span style="color: #009999">2855</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">3.0080</span> - acc: <span style="color: #009999">0.1891</span>     
Epoch <span style="color: #009999">1</span>/<span style="color: #009999">3</span>
<span style="color: #009999">3192</span>/<span style="color: #009999">3192</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">3.7089</span> - acc: <span style="color: #009999">0.1598</span>     
Epoch <span style="color: #009999">2</span>/<span style="color: #009999">3</span>
<span style="color: #009999">3192</span>/<span style="color: #009999">3192</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">3.6826</span> - acc: <span style="color: #009999">0.1526</span>     
Epoch <span style="color: #009999">3</span>/<span style="color: #009999">3</span>
<span style="color: #009999">3192</span>/<span style="color: #009999">3192</span> [==============================] - <span style="color: #009999">2</span>s - loss: <span style="color: #009999">3.7564</span> - acc: <span style="color: #009999">0.1507</span>     
<span style="color: #009999">0.8629</span>
</pre></div>
<p>Here, we see that our ROC is now 0.8629, which is much better than that of the model that used 5 epochs.</p>
<p>We note that there is a lof of variability in model performance. We noticed that our ROC varied from 0.65 to 0.95. Thus, we have re run our above model generation code multiple times and have saved the best performing model. Below we save these best performing models:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">models_1[<span style="color: #009999">3</span>][<span style="color: #009999">0</span>].save(<span style="color: #aa5500">&#39;model_current_e.h5&#39;</span>)
models_1[<span style="color: #009999">3</span>][<span style="color: #009999">1</span>].save(<span style="color: #aa5500">&#39;model_current_f.h5&#39;</span>)
</pre></div>
<p>We can now load the best performing models:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">model_1 = load_model(<span style="color: #aa5500">&#39;model_current_e.h5&#39;</span>)
model_2 = load_model(<span style="color: #aa5500">&#39;model_current_f.h5&#39;</span>)
</pre></div>
<h1>Model Performance:</h1>
<p>We can now call our functions to obtain the ROC, false positive rate, true positive rate. We call our function with our models as follows:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">roc, fpr, tpr = predict_results(model_1, model_2)

&gt;&gt;&gt;
<span style="color: #009999">0.9476</span>
</pre></div>
<p>Here we see that we have achieved a great ROC of 0.9476.</p>
<p>Let us now plot our ROC curve:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">plot_roc_auc_curve(fpr, tpr, roc, <span style="color: #aa5500">&#39;English/French Classification&#39;</span>)

&gt;&gt;&gt;
<center><img src="../images/roc.png"></center>
</pre></div>
<p>We see that the above ROC curve looks great.</p>

<h1>Questions</h1>
<ul>
<li><b>Is this model good?</b> We see that our model had an ROC of 0.9476. This is a very high ROC and it is a very good model. However, we did intense parameter tuning to arrive at this ROC.</li>
<li><b>What are 3 Alternatives to language detection with their pros and cons?</b>
<ul>
<li></li>
<li></li>
<li></li>
</ul>
</li>
<li><b>What are 5 ways in which we could improve our model?</b>
<ol>
<li><b>More Data </b> - Having more training and testing data is always better, espeically when we are using deep learning. We had 3569 total English sentences to work with and 3990 French sentences to work with. This is not a huge dataset. Having more data from each dataset would allow us to better train our model to better represent the actual population of the data. If we had more text that looked like English or French our model would be able to better understand the English features compared to the French features. However, more data would increase the computational expense for our model. This task already had some expense to it and adding multiple times the amount of data would drastically increase the runtime. We believe that having more data would drop our ROC slightly as we would better be able to represent the English and French populations, but our computational expense would be mich higher.</li>
<li><b>Alternate Layers</b> - Our model had an LSTM layer and a dense layer. Here, we did not explore the possibility of adding alternate layers such as a dropout layer. A droput layer is a powerful regularization technique. Here, neurons that are randomly selected are ignored during training. This allows our network to be less sensitive to the specific weights of neurons. Thus, our network is capable of better generalization and is less likely to overfit the training data. Less overfitting is definitely a pro. Adding and experimenting with different layers can help improve performance, but a con is that this would add computational expense to our program in terms of runtime and in human labor as we would have to find the optimal hyperparameters for the layers. Also, this is not guranteed to help in terms of performance. It may very well be that our network performs worse with a dropout layer or a different layer structure. We believe that adding a dropout layer would prevent overfitting and that experimenting with layers could boost performance in terms of accuracy, precision, recall, and AUC.</li>
<li><b>Different Optimizer</b> - In our model we used an RMSprop optimizer with a learning rate of 0.01. We did not try alternate optimization techniques such as SGD, Adagrad, Adadelta, Adam , etc. With regards to Long Short-Term Memory, Adam and Adadelta usually are a fine choice. We tuned the learning rate for the RMSprop, but did not experiment with these alternate optimizers. Potentially, we could have achieved better performance or converged more quickly than with our choice of optimizer. However, it is not guranteed that a different optimizer would drastically improve the performance. Also, it might take longer to converge than our choice of optimizer. Additionally, running experiments with these would take additional time. We do not think that the optimizer choice would make a huge difference in performance, but we do think that some optimizers are better suited for tasks than others. Thus, we think that we could converge faster using an optimizer such as Adam. This would allow us to run less epochs and decrease our computational expense.</li>
<li><b>Bias</b> - Adding a bias of 1 has been shown to help improve the performance and prevent overfitting especially in regards to LSTM models. When we created our LSTM in keras we did not set use_bias = True. Having this bias vector could have lead to less overfitting in our model. A con would be that our ROC could potentially drop and our model would not look as good as it does. We think that adding bias is a great idea, which would help prevent our model from overfitting, but would drop our ROC. In this case this is fine because we want user querys to be as accurate as possible. Using a bias could potentially achieve this.</li>
<li><b>More Epochs</b> - We used a maximum of 12 epochs when we trained our model. Using a GPU allows for more training in a smaller time frame. Instead of running 12 epochs we could have run 100. Running this experiment and obtaining shorter run time is definitely a positive, but a con would be that the model converges after a few iterations and there is no gained preformance by using the GPU to run more epochs. We think that we could run more epochs, but we would have no change in efficacy. We saw that there was convergence across 8-10 epochs as the derivative of the loss started became somwhat constant from iteration to iteration. However, GPUs allow for much faster training.</li>
</ol> 
</li>
</ul>

<h1>Extra Credit:</h1>
<p>In extension to our previously developed language detection model for English vs French, we will now explore a 10-way langauge comparison of both traditional western langauges as well as non-Latin based langauges. The languages we are utilizing are as follows:</p>
<ul>
<li>English</li>
<li>French</li>
<li>Dutch</li>
<li>German</li>
<li>Italian</li>
<li>Latin</li>
<li>Portugese</li>
<li>Japanese</li>
<li>Russian</li>
<li>Spanish</li>
</ul>
<p>We will create LSTM models for each language and tune the set of optimal hyperparameters found in the English-French LSTM comparison in the previous notebook. There, we evaluated learning rate, decay, batch size, LSTM layer size, and number of epochs. In order to train our 10 models efficiently we spun up a Microsoft Azure NC12 which has 12 cores, 120 GiB ram, and 2 K80 GPUs. This will allow us to run over 2000 epochs in less than two hours. After doing so we will examine the ROC for pairs of language models and be able to determine which paris of langauges our model was able to distinguish best and which languages our models were not able to distingush.</p>

<h1>Imports</h1>
<p>We import the following libraries:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">__future__</span> <span style="color: #0000aa">import</span> print_function;
<span style="color: #0000aa">import</span> <span style="color: #00aaaa; text-decoration: underline">re</span>;
<span style="color: #0000aa">import</span> <span style="color: #00aaaa; text-decoration: underline">sys</span>;
<span style="color: #0000aa">import</span> <span style="color: #00aaaa; text-decoration: underline">random</span>;
<span style="color: #0000aa">import</span> <span style="color: #00aaaa; text-decoration: underline">unicodedata</span>;
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">random</span> <span style="color: #0000aa">import</span> randint;

<span style="color: #0000aa">import</span> <span style="color: #00aaaa; text-decoration: underline">numpy</span> <span style="color: #0000aa">as</span> <span style="color: #00aaaa; text-decoration: underline">np</span>;
<span style="color: #0000aa">import</span> <span style="color: #00aaaa; text-decoration: underline">seaborn</span> <span style="color: #0000aa">as</span> <span style="color: #00aaaa; text-decoration: underline">sns</span>;

<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">keras.models</span> <span style="color: #0000aa">import</span> Sequential;
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">keras.layers</span> <span style="color: #0000aa">import</span> Dense, Activation;
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">keras.layers</span> <span style="color: #0000aa">import</span> LSTM;
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">keras.optimizers</span> <span style="color: #0000aa">import</span> RMSprop;
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">keras.utils.data_utils</span> <span style="color: #0000aa">import</span> get_file;
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">sklearn.cross_validation</span> <span style="color: #0000aa">import</span> train_test_split;
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">sklearn.metrics</span> <span style="color: #0000aa">import</span> *;
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">sklearn.externals</span> <span style="color: #0000aa">import</span> joblib;

<span style="color: #0000aa">import</span> <span style="color: #00aaaa; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #0000aa">as</span> <span style="color: #00aaaa; text-decoration: underline">plt</span>;
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">IPython.display</span> <span style="color: #0000aa">import</span> clear_output
<span style="color: #0000aa">from</span> <span style="color: #00aaaa; text-decoration: underline">keras.callbacks</span> <span style="color: #0000aa">import</span> ModelCheckpoint, Callback

<span style="color: #0000aa">import</span> <span style="color: #00aaaa; text-decoration: underline">subprocess</span>;
<span style="color: #0000aa">import</span> <span style="color: #00aaaa; text-decoration: underline">h5py</span>;
</pre></div>

    <h1>Notifications:</h1>
    <p>Since we will be running many epochs across many different languages we have set up a slack channel to push notifications when functions finish running. This will help monitor the status of our program. We create a function called notify_slack which takes in a text to be sent to our channel. We create it as follows:</p>
    <!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">def</span> <span style="color: #00aa00">notify_slack</span>(text):
    text = <span style="color: #aa5500">&#39;WebSearch: &#39;</span> + text;
    subprocess.Popen(<span style="color: #aa5500">&#39;&#39;&#39;curl -X POST --data-urlencode &quot;payload={&#39;channel&#39; : &#39;#random&#39;, &#39;username&#39;: &#39;webhookbot&#39;, &#39;text&#39;:&#39;&#39;&#39;</span>+ <span style="color: #aa5500">&#39;\&#39;&#39;</span> + text + <span style="color: #aa5500">&#39;\&#39;&#39;</span> + <span style="color: #aa5500">&#39;&#39;&#39;}&quot; https://hooks.slack.com/services/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&#39;&#39;&#39;</span>, shell=<span style="color: #00aaaa">True</span>)
</pre></div>

<h1>Data Cleaning:</h1>
<p>Our data might originally be messy and contain accents that are not held within English alpha-numeric characters. Thus, we will create a function called strip_accents which takes in a string to remove these characters to avoid giving cerain models an unfair advantage. Inside this function we use the unicodedata.normalize function on our given strings with an appropriate code. We do so as follows:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">def</span> <span style="color: #00aa00">strip_accents</span>(s):
    <span style="color: #0000aa">return</span> <span style="color: #aa5500">&#39;&#39;</span>.join(c <span style="color: #0000aa">for</span> c <span style="color: #0000aa">in</span> unicodedata.normalize(<span style="color: #aa5500">&#39;NFD&#39;</span>, s)
                   <span style="color: #0000aa">if</span> unicodedata.category(c) != <span style="color: #aa5500">&#39;Mn&#39;</span>)
</pre></div>

<h1>Loading Data:</h1>
<p>In order to use languages with a non-Latin scrip, we first need to transliterate them into traditional Enlgish. We've used <a href="http://www.lexilogos.com/keyboard/russian_conversion.htm">this link</a> to Romanize Russian and <a href="http://nihongo.j-talk.com/">this link</a> to Romanize Japanese. Additionally, we stripped out these non-translated characters: , , , . We can now load our language datasets. Here we first create a list of all our langauge txt files. Additionally, we create a list with their respective names. We'll keep languagaes in this order for all of our datasets for simplicity. We strip accents, lower case each character and add it to an overall list of languages.</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">languages = [<span style="color: #aa5500">&#39;data/eng.txt&#39;</span>,
             <span style="color: #aa5500">&#39;data/frn.txt&#39;</span>,
             <span style="color: #aa5500">&#39;languages/dut.txt&#39;</span>,
             <span style="color: #aa5500">&#39;languages/ger.txt&#39;</span>, 
             <span style="color: #aa5500">&#39;languages/itn.txt&#39;</span>, 
             <span style="color: #aa5500">&#39;languages/ltn.txt&#39;</span>, 
             <span style="color: #aa5500">&#39;languages/por.txt&#39;</span>, 
             <span style="color: #aa5500">&#39;languages/romanized_jap.txt&#39;</span>, 
             <span style="color: #aa5500">&#39;languages/romanized_rus.txt&#39;</span>, 
             <span style="color: #aa5500">&#39;languages/spn.txt&#39;</span> ]

language_names = [<span style="color: #aa5500">&#39;English&#39;</span>,
                  <span style="color: #aa5500">&#39;French&#39;</span>,
                  <span style="color: #aa5500">&#39;Dutch&#39;</span>,
                  <span style="color: #aa5500">&#39;German&#39;</span>,
                  <span style="color: #aa5500">&#39;Italian&#39;</span>,
                  <span style="color: #aa5500">&#39;Latin&#39;</span>,
                  <span style="color: #aa5500">&#39;Portugese&#39;</span>,
                  <span style="color: #aa5500">&#39;Japanese&#39;</span>,
                  <span style="color: #aa5500">&#39;Russian&#39;</span>,
                  <span style="color: #aa5500">&#39;Spanish&#39;</span>]

all_text = []
<span style="color: #0000aa">for</span> <span style="color: #00aaaa">file</span> <span style="color: #0000aa">in</span> languages:
    text = <span style="color: #00aaaa">open</span>(<span style="color: #00aaaa">file</span>).read().lower()
    all_text.append(strip_accents(text))

<span style="color: #0000aa">for</span> text <span style="color: #0000aa">in</span> all_text:
    <span style="color: #0000aa">print</span>(text[<span style="color: #009999">0</span>:<span style="color: #009999">20</span>])

&gt;&gt;&gt;
universal declaratio
declaration universe
universele verklarin
resolution <span style="color: #009999">217</span> a (ii
il <span style="color: #009999">10</span> dicembre <span style="color: #009999">1948</span>,
declarationem homini
declaracao universal
<span style="color: #aa5500">&#39;sekai jinken sengen</span>
vseobsaja deklaracij
declaracion universa
</pre></div>

<h1>Character Set & Dictionary</h1>
<p>We now need to obtain a set of all of the characters in our language, a dictionary that maps each character to an integer, and a dictionary that maps integers to the characters. To do so we can iterate over all of our languages text in all_data and create a set of the characters in the text for each language. We can then append this into all_chars. In all_char_map we can create a dictionary with the keys as the elements in the sorted set and the values as integers from 0 to len(sorted_characters). We can then append this dictionary into the list all_char_map. We then swap the keys and values in the all_char_map dictionary and append this into all_char_map_inverse. We do so as follows:</p>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">all_chars = []
all_char_map = []
all_char_map_inverse = []
<span style="color: #0000aa">for</span> idx, lang <span style="color: #0000aa">in</span> <span style="color: #00aaaa">enumerate</span>(all_text):
    all_chars.append(<span style="color: #00aaaa">sorted</span>(<span style="color: #00aaaa">list</span>(<span style="color: #00aaaa">set</span>(lang))))
    all_char_map.append(<span style="color: #00aaaa">dict</span>((c, i) <span style="color: #0000aa">for</span> i, c <span style="color: #0000aa">in</span> <span style="color: #00aaaa">enumerate</span>(<span style="color: #00aaaa">sorted</span>(<span style="color: #00aaaa">list</span>(<span style="color: #00aaaa">set</span>(lang))))))
    all_char_map_inverse.append(<span style="color: #00aaaa">dict</span>((i, c) <span style="color: #0000aa">for</span> i, c <span style="color: #0000aa">in</span> <span style="color: #00aaaa">enumerate</span>(<span style="color: #00aaaa">sorted</span>(<span style="color: #00aaaa">list</span>(<span style="color: #00aaaa">set</span>(lang))))))
    
    <span style="color: #0000aa">print</span> (language_names[idx])
    <span style="color: #0000aa">print</span> (<span style="color: #aa5500">&quot;\tCorpus length:&quot;</span>, <span style="color: #00aaaa">len</span>(lang))
    <span style="color: #0000aa">print</span> (<span style="color: #aa5500">&quot;\tCharacter Count&quot;</span>, <span style="color: #00aaaa">len</span>(all_chars[idx]))
    <span style="color: #0000aa">print</span> ()

&gt;&gt;&gt; 
English
    Corpus length: <span style="color: #009999">10746</span>
    Character Count <span style="color: #009999">43</span>

French
    Corpus length: <span style="color: #009999">12009</span>
    Character Count <span style="color: #009999">41</span>

Dutch
    Corpus length: <span style="color: #009999">12887</span>
    Character Count <span style="color: #009999">42</span>

German
    Corpus length: <span style="color: #009999">12078</span>
    Character Count <span style="color: #009999">45</span>

Italian
    Corpus length: <span style="color: #009999">12768</span>
    Character Count <span style="color: #009999">39</span>

Latin
    Corpus length: <span style="color: #009999">10065</span>
    Character Count <span style="color: #009999">32</span>

Portugese
    Corpus length: <span style="color: #009999">11459</span>
    Character Count <span style="color: #009999">42</span>

Japanese
    Corpus length: <span style="color: #009999">11551</span>
    Character Count <span style="color: #009999">38</span>

Russian
    Corpus length: <span style="color: #009999">12240</span>
    Character Count <span style="color: #009999">43</span>

Spanish
    Corpus length: <span style="color: #009999">12078</span>
    Character Count <span style="color: #009999">42</span>
</pre></div>

<p>We can now create our sentences like we previously did for our English and French models:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">maxlen = <span style="color: #009999">40</span>
step = <span style="color: #009999">3</span>

all_sentences = []
all_next_chars = []

<span style="color: #0000aa">for</span> idx, lang <span style="color: #0000aa">in</span> <span style="color: #00aaaa">enumerate</span>(all_text):
    sentences = []
    next_chars = []
    <span style="color: #0000aa">for</span> i <span style="color: #0000aa">in</span> <span style="color: #00aaaa">range</span>(<span style="color: #009999">0</span>, <span style="color: #00aaaa">len</span>(lang) - maxlen, step):
        sentences.append(lang[i: i + maxlen])
        next_chars.append(lang[i + maxlen])
    
    all_sentences.append(sentences)
    all_next_chars.append(next_chars)

    <span style="color: #0000aa">print</span> (language_names[idx])
    <span style="color: #0000aa">print</span> (<span style="color: #aa5500">&quot;\tSentences:&quot;</span>, <span style="color: #00aaaa">len</span>(sentences))
    <span style="color: #0000aa">print</span> ()

&gt;&gt;&gt;
English
    Sentences: <span style="color: #009999">3569</span>

French
    Sentences: <span style="color: #009999">3990</span>

Dutch
    Sentences: <span style="color: #009999">4283</span>

German
    Sentences: <span style="color: #009999">4013</span>

Italian
    Sentences: <span style="color: #009999">4243</span>

Latin
    Sentences: <span style="color: #009999">3342</span>

Portugese
    Sentences: <span style="color: #009999">3807</span>

Japanese
    Sentences: <span style="color: #009999">3837</span>

Russian
    Sentences: <span style="color: #009999">4067</span>

Spanish
    Sentences: <span style="color: #009999">4013</span>
</pre></div>

    <h1>Design Matrix Creation</h1>
    <p>We will now vectorize the above matrices by creating a design matrix. This will result in an [x, 40, max-alphabet-length] shape matrix for each langauge. We do so like we did for the English and French models:</p>
    <!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">char_len = <span style="color: #00aaaa">max</span>([<span style="color: #00aaaa">len</span>(x) <span style="color: #0000aa">for</span> x <span style="color: #0000aa">in</span> all_chars])

all_x = []
all_y = []

<span style="color: #0000aa">for</span> idx, lang <span style="color: #0000aa">in</span> <span style="color: #00aaaa">enumerate</span>(all_text):
    x = np.zeros((<span style="color: #00aaaa">len</span>(all_sentences[idx]), maxlen, char_len), dtype=np.bool)
    y = np.zeros((<span style="color: #00aaaa">len</span>(all_sentences[idx]), char_len), dtype=np.bool)
    
    <span style="color: #0000aa">for</span> i, sentence <span style="color: #0000aa">in</span> <span style="color: #00aaaa">enumerate</span>(all_sentences[idx]):
        <span style="color: #0000aa">for</span> t, char <span style="color: #0000aa">in</span> <span style="color: #00aaaa">enumerate</span>(sentence):
            x[i, t, all_char_map[idx][char]] = <span style="color: #009999">1</span>
        y[i, all_char_map[idx][all_next_chars[idx][i]]] = <span style="color: #009999">1</span>
    
    all_x.append(x)
    all_y.append(y)
</pre></div>

    <h1>Train/Test Split:</h1>
    <p>We now use sklearn's train_test_split function to create an 80% training set and 20% testing set for our data.</p>
    <!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">all_train_x = []
all_test_x = []
all_train_y = []
all_test_y =[]

<span style="color: #0000aa">for</span> idx, lang <span style="color: #0000aa">in</span> <span style="color: #00aaaa">enumerate</span>(all_text):
    train_x, test_x, train_y, test_y = train_test_split(all_x[idx], all_y[idx], test_size=<span style="color: #009999">0.2</span>, random_state=<span style="color: #009999">1024</span>);
    all_train_x.append(train_x)
    all_test_x.append(test_x)
    all_train_y.append(train_y)
    all_test_y.append(test_y)
</pre></div>

<h1>Generate Testing Data:</h1>
<p>Having obtained the test split, we need to obtain a smaller sample of 100 strings of 5 length charachters each. We do this by using a Random number generator to select a random string each time, append it to our features array, along with the label associated with it.
In orde to compare two languages, we had to make changes to the language generation method. We take two random seeds to create random generators. We seed them in such a way to ensure that the language in the first pair has the same text for each language it is compared to and similarly for the second pair. This also ensures that when we compare English to English, we use two different test samples.
For example, the same English text will be compared to all other languages. Then when we compare all other languages to English we still use the same English text sample. This ensures the maximum variability in our language detection scheme.</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">def</span> <span style="color: #00aa00">random_generate</span>(test_x_1, test_x_2, seed_1, seed_2):
    both_labels = []
    both_feats = []
    
    rands = [random.Random(), random.Random()]

    rands[<span style="color: #009999">0</span>].seed(seed_1)
    rands[<span style="color: #009999">1</span>].seed(sys.maxsize - seed_2)
    
    
    key = <span style="color: #009999">1</span>
    <span style="color: #0000aa">for</span> test_x <span style="color: #0000aa">in</span> [test_x_1, test_x_2]:
        labels = []
        feats = []
        <span style="color: #0000aa">for</span> i <span style="color: #0000aa">in</span> <span style="color: #00aaaa">range</span>(<span style="color: #009999">100</span>): 
            r1 = rands[key].randint(<span style="color: #009999">0</span>, <span style="color: #00aaaa">len</span>(test_x) - <span style="color: #009999">1</span>)
            
            ind = test_x[r1]
            r2 = rands[key].randint(<span style="color: #009999">0</span>, <span style="color: #00aaaa">len</span>(ind) - <span style="color: #009999">5</span>)

            sub_string = ind[r2:r2+<span style="color: #009999">5</span>]

            feats.append(sub_string)
            labels.append(key)
            
        both_labels.append(labels)
        both_feats.append(feats)
        key = key^<span style="color: #009999">1</span>
        
    <span style="color: #0000aa">return</span> both_feats, both_labels
</pre></div>

    <p>With out above function we can now call it for all pairs of languages and append them into a samples and labels list:</p>
    <!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">all_samples_1 = []
all_labels_1 = []

all_samples_2 = []
all_labels_2 = []

<span style="color: #0000aa">for</span> idx, test_x <span style="color: #0000aa">in</span> <span style="color: #00aaaa">enumerate</span>(all_test_x):
    <span style="color: #0000aa">for</span> idx2, test_x2 <span style="color: #0000aa">in</span> <span style="color: #00aaaa">enumerate</span>(all_test_x):
        
        [[feats_1, feats_2], [labels_1, labels_2]] = random_generate(test_x, test_x2, idx, idx2)
        all_samples_1.append(feats_1)
        all_labels_1.append(labels_1)
        
        all_samples_2.append(feats_2)
        all_labels_2.append(labels_2)
</pre></div>

    <h1>LSTM Model Creation:</h1>
    <p>Now that we have obtained our data for training and testing, we will build a LSTM model. To do this, we will create a function that will build a Sequential LSTM model. The parameters for the model below have been tuned after testing multiple configurations with over 60 epochs each. We settled on using 256 Neurons on the LSTM layer, with a RMSProp optimizer using 0.01 Learning rate. We will show our results on how we obtained these values later.</p>
    <!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">def</span> <span style="color: #00aa00">build_model</span>(chars):
    <span style="color: #0000aa">print</span>(<span style="color: #aa5500">&#39;Build model...&#39;</span>)
    model = Sequential()
    model.add(LSTM(<span style="color: #009999">256</span>, input_shape=(<span style="color: #00aaaa">None</span>, char_len)))
    model.add(Dense(char_len))
    model.add(Activation(<span style="color: #aa5500">&#39;softmax&#39;</span>))
    
    optimizer = RMSprop(lr=<span style="color: #009999">0.01</span>)
    model.compile(loss=<span style="color: #aa5500">&#39;categorical_crossentropy&#39;</span>, optimizer=optimizer, metrics=[<span style="color: #aa5500">&#39;accuracy&#39;</span>]);
    <span style="color: #0000aa">return</span> model
</pre></div>

    <h1>Prediction & Log Likelihood:</h1>
    <p>To do a prediction on our data, we will use a log of probabilities over each charachter in a string. We will iterate over each charachter, obtain the vector associated with it along with the previous START charachters, and obtain the prediction score for it on the next charachter. Finally, we sum up the logs of these values and return it.</p>
    <!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">def</span> <span style="color: #00aa00">predict_on_sample</span>(model, test_val):
    start = np.zeros((<span style="color: #009999">1</span>, <span style="color: #009999">1</span>, char_len), dtype=<span style="color: #00aaaa">bool</span>);
    start_prob = model.predict(start);

    next_vec = start.copy()[<span style="color: #009999">0</span>][<span style="color: #009999">0</span>];
    probs = [];

    probs.append(start_prob[<span style="color: #009999">0</span>,np.argwhere(test_val[<span style="color: #009999">0</span>])[<span style="color: #009999">0</span>][<span style="color: #009999">0</span>]]);

    <span style="color: #0000aa">for</span> idx, vec <span style="color: #0000aa">in</span> <span style="color: #00aaaa">enumerate</span>(test_val):
        next_vec = np.append(next_vec, vec).reshape(<span style="color: #009999">1</span>, idx+<span style="color: #009999">2</span>, char_len)
        next_prob = model.predict(next_vec);

        probs.append(next_prob[<span style="color: #009999">0</span>, np.argwhere(test_val[idx])[<span style="color: #009999">0</span>][<span style="color: #009999">0</span>]]);
        
    <span style="color: #0000aa">return</span> np.sum(np.log(probs));
</pre></div>
    <p>Using the above function, we can predict the results on each test string. To do this, we pass in the string along with both the English model and the French model, obtain the probabilities, and compute the GLRT. The ratio gives us a vector of probability scores, in which we set all values >= 1 to 1, and otherwise 0. The ROC score can then be computed on these values.</p>

    <h1>ROC</h1>
    <p>We can use the same function predict_results that we built in Part 1 to obtain the ROC score, false positive rate, and true positive rate. We do so as follows:</p>
    <!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">def</span> <span style="color: #00aa00">predict_results</span>(model_1, model_2, lang_idx_1, lang_idx_2):
    
    test_data = np.array(all_samples_1[lang_idx_1] + all_samples_2[lang_idx_2]);
    test_labels = np.array(all_labels_1[lang_idx_1] + all_labels_2[lang_idx_2]);
    
    preds_1 = np.array([predict_on_sample(model_1, x) <span style="color: #0000aa">for</span> x <span style="color: #0000aa">in</span> test_data]);
    preds_2 = np.array([predict_on_sample(model_2, x) <span style="color: #0000aa">for</span> x <span style="color: #0000aa">in</span> test_data]);
    
    ratio_probs = preds_1 - preds_2;
        
    fpr, tpr, _ = roc_curve(test_labels, ratio_probs);
    roc_auc = roc_auc_score(test_labels, ratio_probs);

    <span style="color: #0000aa">return</span> roc_auc, fpr, tpr;
</pre></div>

<h1>Hyperparameter Tuning:</h1>
<p>We use the same hyperparameters as we did in the first section. Here we did an entense search wich gave us an ROC of ~0.95 for our English and French models. For more details on this please read the above hyperparameter tuning section.</p>

<h1>Building Models:</h1>
<p>We create a function called train_and_predict to build two models for each respective language, fit the training data, predict the results, and obtain the roc. The function is as follows:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">def</span> <span style="color: #00aa00">train_and_predict</span>(lang_idx_1, lang_idx_2, total_epochs, batch_size=<span style="color: #009999">2048</span>):
    model_1 = build_model(all_chars[lang_idx_1])
    model_2 = build_model(all_chars[lang_idx_2]);

    epochs_ran = <span style="color: #009999">0</span>;
    
    history_1 = model_1.fit(all_train_x[lang_idx_1], all_train_y[lang_idx_1],
                        batch_size=batch_size, epochs=total_epochs, shuffle=<span style="color: #00aaaa">True</span>, verbose=<span style="color: #009999">0</span>);
    history_2 = model_2.fit(all_train_x[lang_idx_2], all_train_y[lang_idx_2],
                        batch_size=batch_size, epochs=total_epochs, shuffle=<span style="color: #00aaaa">True</span>, verbose=<span style="color: #009999">0</span>);

    roc, _, _ = predict_results(model_1, model_2, lang_idx_1, lang_idx_2);

           
    <span style="color: #0000aa">return</span> roc, history_1, history_2, model_1, model_2;
</pre></div>

<p>We can now run each of our 100 models for comparisons between every language. We then save them to a 5-tuple of outputs which we represent as a variable called all_models.</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">all_models = []
<span style="color: #0000aa">for</span> idx, lang <span style="color: #0000aa">in</span> <span style="color: #00aaaa">enumerate</span>(all_text):
    model_results = []
    <span style="color: #0000aa">for</span> idx_2, lang2 <span style="color: #0000aa">in</span> <span style="color: #00aaaa">enumerate</span>(all_text):
        result = train_and_predict(lang_idx_1=idx, lang_idx_2=idx_2, total_epochs=<span style="color: #009999">8</span>, batch_size=<span style="color: #009999">2512</span>)
        model_results.append(result)
        <span style="color: #0000aa">print</span> (<span style="color: #aa5500">&quot;Finished: %s -&gt; %s with ROC=%f&quot;</span> % (language_names[idx], language_names[idx_2], result[<span style="color: #009999">0</span>]))
    all_models.append(model_results)
    notify_slack(<span style="color: #aa5500">&quot;Finished all models for &quot;</span> + language_names[idx])
</pre></div>

<h1>Model Evaluation:</h1>
<p>Next, we extract the loss and the ROC of each language combination. We've printed the ROCs below so they can be read easily and then create a heat map to give a graphical representation of how languages compare to each other.</p>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">all_roc = <span style="color: #00aaaa">dict</span>()
all_roc_list = []

all_fpr = <span style="color: #00aaaa">dict</span>()
all_tpr = <span style="color: #00aaaa">dict</span>()
all_loss = []

<span style="color: #0000aa">for</span> idx_1 <span style="color: #0000aa">in</span> <span style="color: #00aaaa">range</span>(<span style="color: #00aaaa">len</span>(all_models)):
    language_roc = []
    language_loss = []
    
    <span style="color: #0000aa">for</span> idx_2 <span style="color: #0000aa">in</span> <span style="color: #00aaaa">range</span>(<span style="color: #00aaaa">len</span>(all_models[idx_1])):
        roc, history_1, history_2, model_1, model_2 = all_models[idx_1][idx_2]
        roc_auc, fpr, tpr = predict_results(model_1, model_2, idx_1, idx_2)
        
        index = idx_1*<span style="color: #00aaaa">len</span>(all_models) + idx_2
        all_roc[index] = roc_auc
        all_fpr[index] = fpr
        all_tpr[index] = tpr
        
        language_roc.append(roc_auc)
        language_loss.append(history_2.history[<span style="color: #aa5500">&quot;loss&quot;</span>][-<span style="color: #009999">1</span>])
        
        <span style="color: #0000aa">print</span> (<span style="color: #aa5500">&quot;%s - %s ROC: %f&quot;</span> % (language_names[idx_1], language_names[idx_2], roc_auc))
    <span style="color: #0000aa">print</span>()
    
    all_roc_list.append(language_roc)
    all_loss.append(language_loss)

&gt;&gt;&gt;
English - English ROC: <span style="color: #009999">0.477350</span>
English - French ROC: <span style="color: #009999">0.802600</span>
English - Dutch ROC: <span style="color: #009999">0.755800</span>
English - German ROC: <span style="color: #009999">0.908600</span>
English - Italian ROC: <span style="color: #009999">0.810800</span>
English - Latin ROC: <span style="color: #009999">0.998000</span>
English - Portugese ROC: <span style="color: #009999">0.762200</span>
English - Japanese ROC: <span style="color: #009999">0.867500</span>
English - Russian ROC: <span style="color: #009999">0.749200</span>
English - Spanish ROC: <span style="color: #009999">0.882100</span>

French - English ROC: <span style="color: #009999">0.539400</span>
French - French ROC: <span style="color: #009999">0.591300</span>
French - Dutch ROC: <span style="color: #009999">0.540000</span>
French - German ROC: <span style="color: #009999">0.821700</span>
French - Italian ROC: <span style="color: #009999">0.730300</span>
French - Latin ROC: <span style="color: #009999">0.876100</span>
French - Portugese ROC: <span style="color: #009999">0.700300</span>
French - Japanese ROC: <span style="color: #009999">0.619500</span>
French - Russian ROC: <span style="color: #009999">0.839700</span>
French - Spanish ROC: <span style="color: #009999">0.249700</span>

Dutch - English ROC: <span style="color: #009999">0.569900</span>
Dutch - French ROC: <span style="color: #009999">0.427800</span>
Dutch - Dutch ROC: <span style="color: #009999">0.403100</span>
Dutch - German ROC: <span style="color: #009999">0.872900</span>
Dutch - Italian ROC: <span style="color: #009999">0.673000</span>
Dutch - Latin ROC: <span style="color: #009999">0.993700</span>
Dutch - Portugese ROC: <span style="color: #009999">0.280700</span>
Dutch - Japanese ROC: <span style="color: #009999">0.954800</span>
Dutch - Russian ROC: <span style="color: #009999">0.862400</span>
Dutch - Spanish ROC: <span style="color: #009999">0.532500</span>

German - English ROC: <span style="color: #009999">0.419550</span>
German - French ROC: <span style="color: #009999">0.589200</span>
German - Dutch ROC: <span style="color: #009999">0.562500</span>
German - German ROC: <span style="color: #009999">0.275900</span>
German - Italian ROC: <span style="color: #009999">0.791100</span>
German - Latin ROC: <span style="color: #009999">0.960000</span>
German - Portugese ROC: <span style="color: #009999">0.656000</span>
German - Japanese ROC: <span style="color: #009999">0.888800</span>
German - Russian ROC: <span style="color: #009999">0.731600</span>
German - Spanish ROC: <span style="color: #009999">0.634300</span>

Italian - English ROC: <span style="color: #009999">0.523200</span>
Italian - French ROC: <span style="color: #009999">0.764300</span>
Italian - Dutch ROC: <span style="color: #009999">0.718100</span>
Italian - German ROC: <span style="color: #009999">0.851100</span>
Italian - Italian ROC: <span style="color: #009999">0.710600</span>
Italian - Latin ROC: <span style="color: #009999">0.921500</span>
Italian - Portugese ROC: <span style="color: #009999">0.729000</span>
Italian - Japanese ROC: <span style="color: #009999">0.496600</span>
Italian - Russian ROC: <span style="color: #009999">0.859700</span>
Italian - Spanish ROC: <span style="color: #009999">0.719200</span>

Latin - English ROC: <span style="color: #009999">0.459950</span>
Latin - French ROC: <span style="color: #009999">0.774300</span>
Latin - Dutch ROC: <span style="color: #009999">0.474000</span>
Latin - German ROC: <span style="color: #009999">0.582900</span>
Latin - Italian ROC: <span style="color: #009999">0.620700</span>
Latin - Latin ROC: <span style="color: #009999">0.122300</span>
Latin - Portugese ROC: <span style="color: #009999">0.759200</span>
Latin - Japanese ROC: <span style="color: #009999">0.889800</span>
Latin - Russian ROC: <span style="color: #009999">0.722400</span>
Latin - Spanish ROC: <span style="color: #009999">0.687900</span>

Portugese - English ROC: <span style="color: #009999">0.465650</span>
Portugese - French ROC: <span style="color: #009999">0.181500</span>
Portugese - Dutch ROC: <span style="color: #009999">0.440800</span>
Portugese - German ROC: <span style="color: #009999">0.883500</span>
Portugese - Italian ROC: <span style="color: #009999">0.704400</span>
Portugese - Latin ROC: <span style="color: #009999">0.940300</span>
Portugese - Portugese ROC: <span style="color: #009999">0.765300</span>
Portugese - Japanese ROC: <span style="color: #009999">0.581000</span>
Portugese - Russian ROC: <span style="color: #009999">0.762200</span>
Portugese - Spanish ROC: <span style="color: #009999">0.781800</span>

Japanese - English ROC: <span style="color: #009999">0.509550</span>
Japanese - French ROC: <span style="color: #009999">0.491700</span>
Japanese - Dutch ROC: <span style="color: #009999">0.558000</span>
Japanese - German ROC: <span style="color: #009999">0.684700</span>
Japanese - Italian ROC: <span style="color: #009999">0.440500</span>
Japanese - Latin ROC: <span style="color: #009999">0.860400</span>
Japanese - Portugese ROC: <span style="color: #009999">0.719100</span>
Japanese - Japanese ROC: <span style="color: #009999">0.185900</span>
Japanese - Russian ROC: <span style="color: #009999">0.558600</span>
Japanese - Spanish ROC: <span style="color: #009999">0.587150</span>

Russian - English ROC: <span style="color: #009999">0.456500</span>
Russian - French ROC: <span style="color: #009999">0.767000</span>
Russian - Dutch ROC: <span style="color: #009999">0.567500</span>
Russian - German ROC: <span style="color: #009999">0.753800</span>
Russian - Italian ROC: <span style="color: #009999">0.762700</span>
Russian - Latin ROC: <span style="color: #009999">0.789500</span>
Russian - Portugese ROC: <span style="color: #009999">0.485850</span>
Russian - Japanese ROC: <span style="color: #009999">0.912000</span>
Russian - Russian ROC: <span style="color: #009999">0.153200</span>
Russian - Spanish ROC: <span style="color: #009999">0.682300</span>

Spanish - English ROC: <span style="color: #009999">0.553750</span>
Spanish - French ROC: <span style="color: #009999">0.764800</span>
Spanish - Dutch ROC: <span style="color: #009999">0.361900</span>
Spanish - German ROC: <span style="color: #009999">0.912800</span>
Spanish - Italian ROC: <span style="color: #009999">0.567100</span>
Spanish - Latin ROC: <span style="color: #009999">0.953900</span>
Spanish - Portugese ROC: <span style="color: #009999">0.519700</span>
Spanish - Japanese ROC: <span style="color: #009999">0.917700</span>
Spanish - Russian ROC: <span style="color: #009999">0.917300</span>
Spanish - Spanish ROC: <span style="color: #009999">0.342050</span>
</pre></div>

<p>To see these easier we can create a heatmap to display the ROC's for each langauge. We do so as follows:</p>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">sns.set()
ax = sns.heatmap(all_roc_list, vmin=<span style="color: #009999">0</span>, vmax=<span style="color: #009999">1</span>, annot=<span style="color: #00aaaa">True</span>, yticklabels=language_names, xticklabels=language_names)
plt.xticks(rotation=<span style="color: #009999">45</span>)
plt.title(<span style="color: #aa5500">&quot;ROC Heatmap&quot;</span>)
sns.plt.show()

&gt;&gt;&gt;
<center><img src="../images/roc_ec_heatmap.png"></center>
</pre></div>
<p>We can also generate an ROC curve to plot multiple ROC curves on the same graph. This will allow us to visualize all ROC's together:</p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">def</span> <span style="color: #00aa00">plot_roc_auc</span>(fpr_, tpr_, roc_, title, num_plot):
    plt.figure(figsize=(<span style="color: #009999">10</span>,<span style="color: #009999">12</span>))

    language_pairs = []
    <span style="color: #0000aa">for</span> lang <span style="color: #0000aa">in</span> language_names:
        <span style="color: #0000aa">for</span> lang2 <span style="color: #0000aa">in</span> language_names:
            language_pairs.append(lang[<span style="color: #009999">0</span>:<span style="color: #009999">3</span>]+<span style="color: #aa5500">&quot;-&quot;</span>+lang2[<span style="color: #009999">0</span>:<span style="color: #009999">3</span>])
    
    lw=<span style="color: #009999">1.15</span>
    <span style="color: #0000aa">for</span> i <span style="color: #0000aa">in</span> <span style="color: #00aaaa">range</span>(num_plot):
        plt.plot(fpr_[i], tpr_[i], lw=lw,
                 label=<span style="color: #aa5500">&#39;{0} (area = {1:0.2f})&#39;</span>
                 <span style="color: #aa5500">&#39;&#39;</span>.format(language_pairs[i], roc_[i]))

    plt.plot([<span style="color: #009999">0</span>, <span style="color: #009999">1</span>], [<span style="color: #009999">0</span>, <span style="color: #009999">1</span>], <span style="color: #aa5500">&#39;k--&#39;</span>, lw=lw)
    plt.xlim([<span style="color: #009999">0.0</span>, <span style="color: #009999">1.0</span>])
    plt.ylim([<span style="color: #009999">0.0</span>, <span style="color: #009999">1.05</span>])
    plt.xlabel(<span style="color: #aa5500">&#39;False Positive Rate&#39;</span>)
    plt.ylabel(<span style="color: #aa5500">&#39;True Positive Rate&#39;</span>)
    plt.title(<span style="color: #aa5500">&quot;ROC AUC Curve for &quot;</span> + title)
    <span style="color: #aaaaaa; font-style: italic">#plt.legend(loc=9, bbox_to_anchor=(0.5, -0.1))</span>
    plt.legend(bbox_to_anchor=(<span style="color: #009999">1.05</span>, <span style="color: #009999">1</span>), loc=<span style="color: #009999">2</span>, borderaxespad=<span style="color: #009999">0.</span>)

    plt.show()
</pre></div>

<p>Let us now call this function:</p>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">plot_roc_auc(all_fpr, all_tpr, all_roc, <span style="color: #aa5500">&quot;all Languages&quot;</span>, <span style="color: #00aaaa">len</span>(all_fpr))

&gt;&gt;&gt;
<center><img src="../images/multiple_roc.png"></center>
</pre></div>

<p>Because of the large number of lines in the plot, it is hard to discern what the different ROCs are, but we can see a good range of values such as those that are above the line, near the middle, and those on the other side. Let's plot the curves for only English to see a more cleaner plot.</p>

<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">plot_roc_auc(all_fpr, all_tpr, all_roc, <span style="color: #aa5500">&quot;English&quot;</span>, <span style="color: #009999">10</span>)

&gt;&gt;&gt;
<center><img src="../images/eng_ec_roc.png"></center>
</pre></div>

<p>The plot shows that the only line that is around the line is the English-to-English curve, which makes sense because a well trained Neural Network will be unable to distinguish between two samples of the same language.</p>

<h1>Results & Analysis</h1>
<p>The ROC heatmap is a really interesting way to represent the interplay between languages. We can see clear trends in display. First, Latin consitently is the most easily differentiable of the languages with several high values and a max ROC of 0.96 in it's comparision to English. This shows us that the model did a great job of distinquishing between Latin and other languages.</p>
<p>Interestingly, Russian and Japanese are not very well distinguishable. This is very likely due to the Romanization process of the languages. The Romanized forms of these languages did not develop on their own but are artificially created. As a result, they are probably somewhat tame and constrained over a "true" language like Spanish or French.</p>
<p>Another aspect of the matrix is that we should expect the diagonal of the map to be around 0.50 as we cannot easily distinquish between two random samples of a single langugage. We see this trend as expected, minus a few outliers like Italian. This variability is caused by the random sampling used. Thoughout the project, we consistently found that there was a huge amount of variability in performance based on pure-luck. It seems as though one of the sample sets is very "Italian" while the other is not particulary remarkable. As a result, the models can more easily classifiy the samples.
The ROC plots are self explanatory. For English, we see strong performance on all languages excluding the English-English comparison.</p>

<h1>Code</h1>
<p>To obtain the code please refer to this <a href="https://github.com/ad1m/Language_Detection">GitHub repository</a>.




    <h1>References:</h1>
    <ul>
    <li>https://keras.io/getting-started/sequential-model-guide/</li>
    <li>https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py</li>
    </ul>

    <br><br>
    <style>
      .myButton {
        background-color:#44c767;
        -moz-border-radius:14px;
        -webkit-border-radius:28px;
        border-radius:14px;
        border:1px solid #18ab29;
        display:inline-block;
        cursor:pointer;
        color:#ffffff;
        font-family:Arial;
        font-size:17px;
        padding:16px 31px;
        text-decoration:none;
        text-shadow:0px 1px 0px #2f6627;
      }
    .myButton:hover {
      background-color:#5cbf2a;
      }
    .myButton:active {
      position:relative;
      top:1px;
      }
  </style>
  <a href="../index.html" class="myButton">$\leftarrow$</a>

    </section>

  </body>
</html>
