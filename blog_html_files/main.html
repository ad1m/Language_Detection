<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Part 1</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="css/normalize.css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="css/cayman.css">
    <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
      });
    </script>
  </head>
  <body>

    <style>
    #right_div {
    float: right;
    border: 0px solid black;
    text-align: center;
}
</style>


    <section class="page-header">
      <br>
      <br>
      <h1 class="project-name">Long Short Term Memory</h1>
      <h2 class="project-tagline">Part 1: Motivation, History, Theory, & Application</h2>
      <h3 class="project-tagline">By: Adam Lieberman, Ravish Chawla, & Garrett Mallory</h3>
      <br>
      <br>
      <!--<a href="#" class="btn">View on GitHub</a>
      <a href="#" class="btn">Download .zip</a>
      <a href="#" class="btn">Download .tar.gz</a> -->
    </section>

    <section class="main-content">
    <h1>Motivation</h1>
   	<p>Suppose you are trying to train a neural network to understand the following sentence:</p>
   	<blockquote>
   		<p>"Ryan drove the car."</p>
   	</blockquote>
   	<p>This sentence clearly has a different connotation from the following sentence:</p>
   	<blockquote>
   		<p>"The car drove Ryan."</p>
   	</blockquote>
   	<p>The human brain immideatly recognizes these difference. We can tell that in the first sentence a human drove the car, but in the second sentence there must be some artifical intelligence going on because the car is driving the human. Our brains have neurons with feedback connections, which can learn many behaviors and sequencing tasks. Thus, we can pick up on these differences very easily because we read each word in the sentence sequentially. We are picking up the context of each word from the words before it. Traditional neural networks would not differentiate the above two sentences, but a special type of network called a Long Short-Term Memory Network, a type of recurrent neural network, would be able to differentiate these two sentences. These types of networks are great for sequential taks like stock market prediction, music generation, speech recognition, and language detection.</p>
    
    <h1>Prerequisites</h1>
    <p>Before we talk about Long Short-Term Memory Networks, let us briefly cover a few prerequisites:</p>
    <h4>Feed Forward Neural Networks:</h4>
    <p>The goal of a feed forward neural network is to approximate some function $f^*$. For example, let $y = f^*(x)$ be a classifier which maps $x$ to a category $y$. The feed forward network defines a mapping $f(x;\theta) \rightarrow y$, which we can express as $f(x;\theta) = y$. This mapping learns the values of parameters of $\theta$, which result in the best approximation to the function $f^*$.</p>
    <p>A feed forward network has inputs passing through hidden layers to generate outputs. Here, signals can only travel in the direction from input to output. The output layer does not affect the same or any other layer. We see an example as follows:</p>
    <center><img src="../images/feed_forward_nn.png" style="width:300px"></center>
    <p>Above, we see that the input passes through the hidden layer which is then connected to the output layer to generate the outputs. The information is fed straight through, from left to right, never touching a given node twice.</p>  

    <h4>Recurrent Neural Networks:</h4>
    <p>Recurrent neural networks (RNNs) are a family of neural networks for processing sequential data, a sequence of values $x^{(1),x^{(2)},...,x^{(t)}}$. In a traditional neural network, like the feed forward network above, we assume that all inputs and outputs are independent of each other. This is not the case for all tasks. If we are building a chat bot and want to predict the next word in reply, we need to know the words that came before it. Essentially, RNNs have memory. Every element in the sequence has the same task performed on it, with the output being dependant on the previous computations. An RNN looks as follows:</p>
    <center><img src="../images/RNN-unrolled.png"></center>
    <p>Here, we see that the sample $x_t$ is generated based on $x_0, x_1, ..., x_{t-1}$ and the output, $h_t$, is dependent on what comes before it. We see that the network forms loops. The first input is passed into its hidden layer and generates some output $h_0$. The second sequential input is passed into its hidden layer and so is the output from $h_0$. The activation function in hidden layer $h_1$ is activated on the second sample and this is then added with $h_0$. This process is then repeated until the last item in the sequence $x_t$. Traditionally, we depict this nature in the following image:</p> 
    <center><img src="../images/RNN-rolled.png" style="height:180px"></center>
    <p>In the 1990's the vanishing gradient problem emerged as a major obstacle to RNN performance. The gradient expresses the change in all weights with respect to the change in error. Thus, not knowing the gradient foes not allow us to adjust the weigths in a direction taht will reduce the error. If we can't do this then the network will not learn. If any quantity is multiplied by a slightly larger quantity in a repeated fashion, the quantity can become very large. This is also true for the reverse case. If we multiply a quantity repeatedly by a quantity less than one, the quantity will become infitesimal. If this is hard to see, imagine that you are a gambler. You keep betting 1 dollar, but win 97 cents on that dollar every bet. You will soon see that this is not sustainable and you will go bankrupt very soon. The layers and timesteps in an RNN relate to each other through multiplication so the derivatives are susceptible to explosion or vanishing. For instance, let us look at multiple applications of the sigmoid in a repeated fashion:</p>
    <center><img src="../images/sigmoid_repeat.png" style="width:400px"></center>
    <p>The sigmoid activation function is a popular activation in RNN's. We see here that the slope of the data becomes negligble and hence undetectable, thus vanishing.</p>

    <h1>History of the LSTM</h1>
    <p>Recurrent Neural Networks were present in the 1990s, however they had a problem. They could not look far back into the past. In 1991, Dr. Jürgen Schmiduber and his former PhD student Sepp Hochreiter proposed a feedback network to overcome the traditional problems of RNNs. Hochreiter showed that deep neural networks were hard to train because they suffer from vanishing or exploding gradients. This was typically the case in RNNs. They either shrunk rapidly or grew wildly out of bounds. In 1997, Schmidhuber and Hochreiter published the paper <a href="http://www.bioinf.jku.at/publications/older/2604.pdf">Long Short-Term Memory</a>. In this paper they reivew Hochreiter's 1991 analysis of the problem of insufficient decaying error back flow in recurrent backpropogation. They combatted this problem by introducing the Long Short-Term Memory (LSTM). In the 90's, computing was still expensive and computing resources were not advanced, so LSTMs were not widely adopted. Fast forward 10 years and services like Amazon AWS and Microsoft Azure offered inexpensive computing which brought massive attention to LSTMs.</p>    
    <h1>What are LSTMs?</h1>
    <p>Long Short-Term Memory networks are a type of recurrent neural network, which overcome the vanishing gradient problem found in a regular RNN. There are multiple variations of LSTMs, but here we will discuss a common form, which consists of the following gates:</p>
    <ul>
    <li><b>Input Gate: </b></li>
    <li><b>Forget Gate: </b></li>
    <li><b>Cell Unit: </b></li>
    </ul>

    

    <h1>Improving Performance of LSTMs:</h1>



    <h1>Applications & Current Research: </h1>
    
    <h1>Research Problems:</h1>
    


  <h1>Further Resources</h1>
  <p>A lot of work has been done on LSTMs. Here are some alternate links worth checking out:</p>
  <h4>Presentations:</h4>
  <ul>
  <li><a href="https://www.youtube.com/watch?v=HN9NRhm9waY">Ian Goodfellow AIWTB 2016 Lecture</a></li>
  <li><a href="https://www.youtube.com/watch?v=RvgYvHyT15E&t=1292s">Ian Goodfellow NIPS 2016 Workshop</a></li>
  <li><a href="https://arxiv.org/pdf/1701.00160.pdf">Ian Goodfellow 2016 NIPS Tutorial</a></li>
  <li><a href="https://www.youtube.com/watch?v=QPkb5VcgXAM&t=657s">Soumith Chintala Facebook London Machine Learning Meetup Lecture</a></li>
  <li><a href="https://www.youtube.com/watch?v=pqkpIfu36Os&index=44&list=PLujxSBD-JXglGL3ERdDOhthD3jTlfudC2">Two Minute Papers Image Editing with GANs</a></li>
  <li><a href="https://www.youtube.com/watch?v=deyOX6Mt_As&t=30s">Siraj Raval Fresh Machine Learning with GANs</a></li>
  </ul>
  <h4>Research Papers:</h4>
  <ul>
  <li><a href="https://arxiv.org/pdf/1406.2661.pdf">2014 Ian Goodfellow Generative Adversarial Nets</a></li>
  <li><a href="https://arxiv.org/pdf/1703.10847.pdf">Music Generation using GANs</a></li>
  <li><a href="https://arxiv.org/pdf/1606.03498v1.pdf">Improved Techniques for Training GANs</a></li>
  <li><a href="https://openreview.net/pdf?id=S1HEBe_Jl">Learning to Protect Communications with Adversarial Neural Cryptography</a></li>
  <li><a href="https://arxiv.org/pdf/1605.05396.pdf">Generative Adversarial Text to Image Synthesis</a></li>
  <li><a href="https://arxiv.org/pdf/1609.04802.pdf">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</a></li>
  <li><a href="http://web.mit.edu/vondrick/tinyvideo/">Generating Videos with Scene Dynamics</a></li>
  <li><a href="https://arxiv.org/pdf/1603.07442v3.pdf">Pixel-Level Domain Transfer</a></li>
  <li><a href"https://arxiv.org/pdf/1511.06434.pdfï%C2%BC‰">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a></li>
  <li><a href="https://pdfs.semanticscholar.org/42f6/f5454dda99d8989f9814989efd50fe807ee8.pdf">Conditional generative adversarial nets for convolutional face generation</a></li>
  <li><a href="https://arxiv.org/pdf/1610.01945.pdf">Connecting Generative Adversarial Networks and Actor-Critic Methods</a></li>
  </ul> 


    <h1>Building our own LSTM:</h1>
    <!--<p>Now that we understand what a generative adversarial network is and how it works, let's proceed by creating our own GAN. We will specifically implement a deep convolutional generative adversarial network (DCGAN) and generate facial images like we described in the motivating example. For this post click the button below.</p> -->
    <style>
      .myButton {
        background-color:#44c767;
        -moz-border-radius:28px;
        -webkit-border-radius:28px;
        border-radius:28px;
        border:1px solid #18ab29;
        display:inline-block;
        cursor:pointer;
        color:#ffffff;
        font-family:Arial;
        font-size:17px;
        padding:16px 31px;
        text-decoration:none;
        text-shadow:0px 1px 0px #2f6627;
      }
    .myButton:hover {
      background-color:#5cbf2a;
      }
    .myButton:active {
      position:relative;
      top:1px;
      }
  </style>
  <center><a href="main2.html" class="myButton">Let's Build an LSTM</a></center>


    <h1>References</h1>
   
    </section>

  </body>
</html>
